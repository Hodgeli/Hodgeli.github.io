<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test_new_begin]]></title>
    <url>%2F2022%2F10%2F05%2Ftest-new-begin%2F</url>
    <content type="text"><![CDATA[测试标题测试正文内容]]></content>
  </entry>
  <entry>
    <title><![CDATA[LeetCode_String]]></title>
    <url>%2F2020%2F03%2F14%2FLeetCode-String%2F</url>
    <content type="text"><![CDATA[14查找字符串数组中的最长公共前缀。 1234567res = ""for i in zip(*strs): if len(set(i)) == 1: res += i[0] else: breakreturn res 效率不如遍历查找来的高 58仅包含大小写字母和空格 ‘ ‘ 的字符串 s中最后一个单词的长度1234return len(s.strip().split(" ")[-1])print("".split(" ")) # ['']print(len("")) # 0 344反转数组1234s.reverse() # 内置方法，C实现，快s[:] = s[::-1] #切片法，略慢于上述头尾指针 逐一交换，python自生实现，最慢 345反转字符串中的元音字母12345678910111213def reverseVowels(self, s: str) -&gt; str: S = list(s) vowel = ["a","e","i","o","u","A","E","I","U","O"] vowel = set(vowel) p1 ,p2 = 0, len(s) - 1 while p1 &lt; p2: while S[p1] not in vowel and p1 &lt; p2: p1 += 1 while S[p2] not in vowel and p1 &lt; p2: p2 -= 1 if p1 &lt; p2: S[p1], S[p2] = S[p2], S[p1] p1 += 1 p2 -= 1 return "".join(S) 49给定一个字符串数组，将字母异位词组合在一起。字母异位词指字母相同，但排列不同的字符串。1234567891011class Solution: def groupAnagrams(self, strs: List[str]) -&gt; List[List[str]]: dict_type_word = &#123;&#125; for word in strs: dict_type_word.setdefault(str(sorted(word)), []).append(word) # 精华所在 list_result = [] for it in dict_type_word.values(): list_result.append(it) return list_result 179给定一组非负整数，重新排列它们的顺序使之组成一个最大的整数。12输入: [3,30,34,5,9]输出: 9534330 12345678910class Solution: def largestNumber(self, nums): """ :type nums: List[int] :rtype: str """ from functools import cmp_to_key temp = list(map(str,nums)) temp.sort(key = cmp_to_key(lambda x,y:int(x+y)-int(y+x)),reverse = True ) return ''.join(temp if temp[0]!='0' else '0') 利用cmp_to_key的返回值辅助sort()方法自定义排序规则 6按照反N型排列字符串，并按行输出形成新的字符串比如输入字符串为 “LEETCODEISHIRING” 行数为 3 时，排列如下：123L C I RE T O E S I I GE D H N 12345678910class Solution: def convert(self, s: str, numRows: int) -&gt; str: if numRows &lt; 2: return s res = ["" for _ in range(numRows)] i, flag = 0, -1 for c in s: res[i] += c if i == 0 or i == numRows - 1: flag = -flag i += flag return "".join(res) res类似节省空间的二维数组，s逐一将字母放置在指定行中。 316给你一个仅包含小写字母的字符串，请你去除字符串中重复的字母，使得每个字母只出现一次。需保证返回结果的字典序最小（要求不能打乱其他字符的相对位置）。1234567891011class Solution: def removeDuplicateLetters(self, s: str) -&gt; str: stack = ['0'] for i, it in enumerate(s): if it not in stack: while it &lt; stack[-1] and stack[-1] in s[i+1:]: stack.pop() stack.append(it) return "".join(stack[1:]) 贪心+栈 65验证给定的字符串是否可以解释为十进制数字。12345-53.5e-93+1.2e+3空格 符号 数字 点 数字 E 符号 数字 空格九种状态 正则表达式解法：12345import reclass Solution: def isNumber(self, s: str) -&gt; bool: pat = re.compile(r'^[\+\-]?(\d+\.\d+|\.\d+|\d+\.|\d+)(e[\+\-]?\d+)?$') return True if len(re.findall(pat, s.strip())) else False 自动机解法：画出状态转换图确定合法的终止状态12345678910111213141516171819202122232425262728293031323334class Solution: def isNumber(self, s: str) -&gt; bool: states = [ &#123;'blank': 0, 'sign': 1, 'digit': 2, 'dot': 3&#125;, &#123;'digit': 2, 'dot': 3&#125;, &#123;'digit': 2, 'dot': 4, 'e': 5, 'blank': 8&#125;, &#123;'digit': 4&#125;, &#123;'digit': 4, 'e': 5, 'blank': 8&#125;, &#123;'sign': 6, 'digit': 7&#125;, &#123;'digit': 7&#125;, &#123;'digit': 7, 'blank': 8&#125;, &#123;'blank': 8&#125; ] current_state = 0 for it in s: if it in " ": it = "blank" elif it in "-+": it = "sign" elif it.isdigit(): it = "digit" elif it == '.': it = "dot" if it not in states[current_state]: return False current_state = states[current_state][it] if current_state not in [2, 4, 7, 8]: return False return True]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[read]]></title>
    <url>%2F2020%2F03%2F09%2Fread%2F</url>
    <content type="text"><![CDATA[什么样的读书是有收获的？ 知道了这本书讲了什么东西，包括核心论点和分论点，论证逻辑和思考框架，论据。做到能够跟朋友大致转述说明的程度； 能够把自己的经历和见解，与书里的观点和理论结合起来，这样形成的反馈才是让人真正理解书中内容的关键； 在以后要用到这本书里面信息的时候，能够快速准确的提取出来使用。 读书容易遇到的问题 慢：无法坚持，无疾而终 忘：看了似乎没看 什么是好的读书笔记？ 记录核心论点 核心论述逻辑 关键论据 认同处 不认同处 后续整理总览分章节总结：可标注将来是否再读]]></content>
      <categories>
        <category>学习方法</category>
      </categories>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sort_out]]></title>
    <url>%2F2020%2F03%2F08%2Fsort-out%2F</url>
    <content type="text"><![CDATA[心理学自我认知如何认识自己 [需求层次理论] 精力管理 《精力管理》，《番茄工作法》 情绪管理 人际交往知乎相关问题 冥想方法论 腹式呼吸法 学习方法论 [学习效率]《学习之道》 [TED] 阅读阅读方法 计算机方法论 [计算机组成原理] 数据结构&amp;算法leetcode 计算机网络《TCP/IP 协议详解》卷一：掌握计算机网络理论知识 《Unix 网络编程》卷一、卷二：网络编程基础知识 操作系统《UNIX 环境高级编程》：掌握基本的 LINUX API 代码&amp;语言《代码大全》、《人月神话》：书写正确的代码 《流畅的python》 C 语言 (C++ 用的少 )，SHELL，PERL，PYTHON 技术方向QEMU，KVM，OPENSTACK，GLUSTERFS，DPDK 如果对开源代码感兴趣，推荐学习下 libevent 这个库，并会使用。 云计算方面，建议同学在 http://www.openstack.org/ 上下载软件自行搭建。学会安装使用，高手 请自行深入源码。 《Redis设计与实现》、《云计算架构技术与实践》 工具Tmux 历史《漫画历史系列》 健身绘画娱乐番剧]]></content>
      <categories>
        <category>整理</category>
      </categories>
      <tags>
        <tag>整理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVTEpro]]></title>
    <url>%2F2020%2F02%2F19%2FCVTEpro%2F</url>
    <content type="text"><![CDATA[北师大123scrapy startproject CVTEpro //新建项目cd CVTEpro\scrapy genspider beijingsd 100875.com.cn //新建爬虫 items.py #数据容器1234567import scrapyclass BeiJingSDItem(scrapy.Item): subject = scrapy.Field() grade = scrapy.Field() file_url = scrapy.Field() #固定字段，用以下载 files = scrapy.Field() #下载完成后scrapy自动填充 beijingsd.py #爬虫文件123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyfrom CVTEpro.items import BeiJingSDItemclass BeijingsdSpider(scrapy.Spider): name = 'beijingsd' allowed_domains = ['100875.com.cn'] start_urls = ['http://www.100875.com.cn/show/js/downloadAnnouncement.js'] # 页面元数据全塞在js中 def parse(self, response): #为方便直接解析手动处理好的js文件 with open("spiders/北师大.txt", encoding='utf8') as f: list_data = f.read() list_data = eval(list_data) for dict_book in list_data: title = dict_book["title"] list_book = dict_book["list"] for book in list_book: item = BeiJingSDItem() item["subject"] = title item['grade'] = book['name'] item['file_url'] = book['url'] yield item pipelines.py #数据操作代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport pymongoimport pymysqlfrom scrapy import Requestfrom scrapy.pipelines.files import FilesPipelineclass BeiJingSDFilePipeline(FilesPipeline): # 电子书籍分层级目录存储 def get_media_requests(self, item, info): file_url = item['file_url'] subject = item['subject'] grade = item['grade'] suffix = file_url.split('.')[-1] filename = "/".join(subject.split("·")) + "/" + grade + '.' + suffix meta = &#123;'filename': filename&#125; # 带路径的文件名 yield Request(url=file_url, meta=meta) def file_path(self, request, response=None, info=None): return request.meta.get('filename', '')class BeiJingSDMongoPipeline(object): # 同步存入MongoDB def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DB') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def process_item(self, item, spider): name = spider.name self.db[name].insert(dict(item)) return item def close_spider(self, spider): self.client.close() settings.py #配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# -*- coding: utf-8 -*-BOT_NAME = 'CVTEpro'SPIDER_MODULES = ['CVTEpro.spiders']NEWSPIDER_MODULE = 'CVTEpro.spiders'ROBOTSTXT_OBEY = False# 是否遵循robots协议FILES_STORE = "BeiJingSDBooks"# 电子书存储路径ITEM_PIPELINES = &#123; # 先下载后存数据库 'CVTEpro.pipelines.BeiJingSDFilePipeline':300, 'CVTEpro.pipelines.BeiJingSDMongoPipeline':301,&#125;MONGO_URI = 'localhost'MONGO_DB = 'BeiJingSDBooks'MY_USER_AGENT = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5", "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre", "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11", "Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",]DOWNLOADER_MIDDLEWARES = &#123; 'CVTEpro.middlewares.MyUserAgentMiddleware': 400,&#125;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XLD]]></title>
    <url>%2F2020%2F02%2F02%2FXLD%2F</url>
    <content type="text"><![CDATA[请在没有人的时候观看]]></content>
  </entry>
  <entry>
    <title><![CDATA[基本库的使用]]></title>
    <url>%2F2019%2F10%2F21%2Furllib%2F</url>
    <content type="text"><![CDATA[urlliburlopen1urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,*, cafile=None, capath=None, cadefault=False, context=None) data12345678910111213print(urllib.parse.urlencode(&#123;"my_key": "my_value"&#125;))# 将键值对编码为可用url传递的格式# my_key=my_value print(urllib.parse.urlencode(&#123;"my_key": &#123;"key_2":"value_2"&#125;&#125;))# 复杂格式的键值对# my_key=%7B%27key_2%27%3A+%27value_2%27%7Ddata = bytes(urllib.parse.urlencode(&#123;"my_key": &#123;"key_2":"value_2"&#125;&#125;), encoding="utf8")# data数据使用前要转化为字节流类型response = urllib.request.urlopen('http://httpbin.org/post',data=data)print(response.read().decode('utf8')) timeouttimeout 参数用于设置超时时间，单位为秒，意思就是如果请求超过了设置的这个时间， 还没有得到响应 就会抛出异常。如果不指定该参数，就会使用全局默认时间它支持 HTTP、HTTPS、FTP请求。 其他参数除了 data 参数和 timeout 参数外，还有 context 参数，它必须是 ssl.SSLContext 类型，用来指定SSL。此外， cafile和capath 这两个参数分别指定 CA 证书和它的路径，这个在请求 HTTPS 链接时有用。cadefault 参数现在已经弃用了，其默认值为 False。 request123request = urllib.request.Request('http://httpbin.org/get')response = urllib.request.urlopen(request)print(response.read().decode('utf8')) 类request(url, data=None, headers={},origin_req_host=None, unverifiable=False,method=None)data:必须为字节流类型headers:请求头，字典。也可用实例的add_header()方法添加origin_req_host:请求方的host名称或IP地址unverifiable:表示这个请求是否是无法验证的，默认是 False ，意思就是说用户没有足够权限来选择接收这个请求的结果 例如，我们请求 HTML 文档中的图片，但是我们没有向自动抓取图像的权限，这时 unverifiable 的值就是 True Handler 验证 12345678910111213141516from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_openerfrom urllib.error import URLErrorusername = 'username'password = 'password'url = 'http://localhost:80/'p = HTTPPasswordMgrWithDefaultRealm()p.add_password(None, url, username, password)auth_handler = HTTPBasicAuthHandler(p)opener = build_opener(auth_handler)try: result = opener.open(url) html = result.read().decode('utf8') print(html)except URLError as e: print(e.reason) 代理 12345678910111213from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_openerproxy_handler = ProxyHandler(&#123; 'http ': 'http://127.0.0.1:9743 ', 'https': 'https://127.0.0.1:9743 '&#125;)opener = build_opener(proxy_handler)try: response = opener.open('https://www.baidu.com') print(response.read().decode('utf-8'))except URLError as e: print(e.reason) Cookies 将网站的 Cookies 获取下来12345678import http.cookiejar, urllib.requestcookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(' http://www.baidu.com')for item in cookie: print(item.name + '=' + item.value) Cookies存文件12345678import http.cookiejar, urllib.requestfilename = 'cookies. txt'cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) 读Cookies123456cookie = http.cookiejar.LWPCookieJar()cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 异常处理URLErrorURLError 类来自 urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request模块生的异常都可以通过捕获这个类来处理它具有一个属性 reason ，即返回错误的原因。 HTTPError它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等 它有如下三个属性code、reason、headers 因为 URLError是HTTPError 的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码更好的写法如下：123456789101112131415import socketfrom urllib import request, errortry: response = request.urlopen('https://cuiqingcai.com/index.htm',timeout=0.1)except error.HTTPError as e: print(e.reason, e.code, e.headers)except error.URLError as e: # 有时候， reason 属性返回的不一定是字符串，也可能是一个对象 if isinstance(e.reason, socket.timeout): print(' TIME OUT') else: print(e.reason)else: print('Request Successfully') 解析链接urlparse该方法可以实现 URL 的识别和分段scheme://netloc/path;params?query#fragment urlunparse有了 urlparse （）， 地就有了它的对立方法 urlunparse （） 它接受的参数是一个可迭代对象，但是它的长度必须是61234from urllib.parse import urlunparsedata = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment' ]print(urlunparse(data)) 其他12345678urlspliturlunspliturljoinurlencode:参数序列化适应GET请求parse_qs:反序列化为字典parse_qsl:反序列化为元组组成的列表quto:将内容转化为 URL 编码的格式unquto Robots协议123User-agent: * (允许的爬虫名称，至少得有一个)Disallow: (/表根目录，禁止所有访问)Allow: /public/ robotparser12345678from urllib.robotparser import RobotFileParserrp = RobotFileParser('http://example.webscraping.com/robots.txt')rp.read()print(rp.can_fetch('BadCrawler', 'http://example.webscraping.com'))# Falseprint(rp.can_fetch('GoodCrawler', 'http://example.webscraping.com'))# True requestsget获取图片、音频、视频二进制存文件即可12345import requestsr = requests.get('https://github.com/favicon.ico')with open('cc.ico','wb') as f: f.write(r.content) 高级用法文件上传12345import requestsfiles = &#123;'file': open('favicon.ico', 'rb ')&#125;r = requests.post('http://httpbin.org/post', files=files)print(r.text) Cookies可直接添加在headers里面或者123jar = requests.cookies.RequestsCookieJar()jar.set(key, value)r = requests.get(url,cookies=jar,headers =headers) 会话维持等同于一个浏览器多个标签页即同一个Cookies123456import requestss = requests.Session()s.get('http://httpbin.org/cookies/set/number/123456789')r = s.get('http://httpbin.org/cookies')print(r.text) SSL 证书验证当遇到证书错误时修改verify参数忽略，但仍会获得一个警告response = requests.get(&#39;https://www.12306.cn&#39;,verify=False)可以忽略警告12from requests.packages import urllib3 urllib3.disable_warnings() 或者通过捕获警告到日志的方式忽略警告12import logging logging.captureWarnings(True) 或使用本地证书（略） 代理设置12345678910111213141516171819import requestsimport randomproxy = [ &#123; 'http': 'http://61.135.217.7:80', 'https': 'http://61.135.217.7:80', &#125;, &#123; 'http': 'http://118.114.77.47:8080', 'https': 'http://118.114.77.47:8080', &#125;, &#123; 'http': 'http://112.114.31.177:808', 'https': 'http://112.114.31.177:808', &#125;]url = '爬取链接地址'response = requests.get(url, proxies=random.choice(proxy)) 若代理需要使用 HTTP Basic Auth ，可以使用类似 http://user:password@host:port 这样的语法来置代理SOCKS 协议代理(略) 超时设置response = requests.get(url, timeout=(5,13))元组代表“连接”与“读取”超时留空或None代表永久等待 身份认证auth=HTTPBasicAuth (’username &#39;,’password&#39;)或者auth ＝（’ username ，’ password&#39;)或者 OAuth 认证pip3 install requests oauthlib Prepared Request各个参数都可以通过一个Request象来表示]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫基础库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Crawler_Foundation]]></title>
    <url>%2F2019%2F10%2F18%2FCrawler-Foundation%2F</url>
    <content type="text"><![CDATA[HTTP基础URL与URIURI = URL + URN其中URL与URN有交集URL = Universal Resource Locator 统一资源定位符,例如： http://example.org/absolute/URI/with/absolute/path/to/resource.txt ftp://example.org/resource.txt 请求请求头 Cookie：也常用复数形式 Cookies ，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是 Cookies 的功能。 Cookies 里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上 Cookies 并将其发送给服务器，服务器通过 Cookies 识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容； Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等； User-Agent：简称 UA ，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本，浏览器及版本等信息 在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别为爬虫 请求体 Content-Type 提交数据的方式 application/x-www-form-urlencoded 表单数据 multipart/form-data 表单文件上传 application/json 序列化JSON数据 text/xml XML数据 响应响应头 Server：务器的信息，比如名称、版本号等 Content-Type：文档类型，指定返回的数据类型是什么，如 text/html 代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript文件， image/jpeg则代表返回图片 Set-Cookie：设置 Cookies。响应头中的 Set-Cookie 告诉浏览器需要将此内容放在 Cookies中，下次请求携带 Cookies 请求。 网页基础HTML定义了网页的内容和结构，css描述了网页的布局，JavaScript定义了网页的行为。 爬虫原理会话与Cookies由于关闭浏览器不会导致会话被删除，这就需要服务器为会话设置一个失效时间，当距离客户端上一次使用会话的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把会话删除以节省存储空间。 代理的基本原理 高度匿名代理： 会将数据包原封不动地转发，在服务端看来就好像真的是 个普通客户端访问，而记录的 IP 是代理服务器的 IP 普通匿名代理： 会在数据包上做一些改动 服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实IP 透明代理： 不但改动了数据包 还会告诉服务器客户端的真实 IP 这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网的硬件防火墙 间谍代理： 指组织或个人创建的用于记录用户传输的数据，然后进行研究监控目的的代理服务器]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>HTTP基础</tag>
        <tag>网页基础</tag>
        <tag>爬虫原理</tag>
        <tag>会话与Cookies</tag>
        <tag>代理的基本原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode_Array]]></title>
    <url>%2F2019%2F07%2F11%2FLeetCode-Array%2F</url>
    <content type="text"><![CDATA[EasyArray Partition I给定一个2n整数的数组，你的任务是把这些整数分成n对整数，比如（a1，b1），（a2，b2），…，（an，bn），这使得min（ai， bi）的和尽可能大。 思路排序，再以步长为2遍历数组，取每组中的较小值相加。代码：12345678910111213class Solution&#123; public int arrayPairSum(int[] nums) &#123; Arrays.sort(nums); //自带排序比我写的快排还好用。。。。。在此场景下 int res = 0; for (int i = 0; i &lt; nums.length; i += 2) &#123; res += nums[i]; &#125; return res; &#125;&#125; Toeplitz Matrix如果从左上角到右下角的每个对角线具有相同的元素，则矩阵是Toeplitz。现在给出一个M×N矩阵，当且仅当矩阵是Toeplitz时才返回True。 思路一对于二维数组（即矩阵）int[][] matrix遍历第一行和第一列的元素，分别向右下角扩散，依次判断每条对角线元素是否全相等。代码1234567891011121314151617181920212223242526272829public class ToeplitzMatrix&#123; public boolean isToeplitzMatrix(int[][] matrix) &#123; for (int j = 0; j &lt; matrix[0].length - 1; j++) //遍历行 &#123; int y = j; for (int x = 0; x &lt; matrix.length - 1 &amp;&amp; y &lt; matrix[0].length - 1; x++, y++) &#123; if (matrix[x][y] != matrix[x + 1][y + 1]) &#123; return false; &#125; &#125; &#125; for (int i = 1; i &lt; matrix.length - 1; i++) //遍历列 &#123; int x = i; for (int y = 0; y &lt; matrix[0].length - 1 &amp;&amp; x &lt; matrix.length - 1; x++, y++) &#123; if (matrix[x][y] != matrix[x + 1][y + 1]) &#123; return false; &#125; &#125; &#125; return true; &#125;&#125; 27. 移除元素给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。示例 1:给定 nums = [3,2,2,3], val = 3,函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。你不需要考虑数组中超出新长度后面的元素。示例 2:给定 nums = [0,1,2,2,3,0,4,2], val = 2,函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。注意这五个元素可为任意顺序。你不需要考虑数组中超出新长度后面的元素。 思路双指针，一头一尾，头指针遇到val后，尾指针左移到第一个不等于val的位置，尾指针的值赋予头指针，头指针右移，尾指针左移。 代码1234567891011121314151617181920class Solution(object): def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ i = 0 # 头指针 n = len(nums)-1 # 尾指针 while(i&lt;=n): if nums[i] == val: while nums[n] == val and n &gt;= 0: # 尾指针向前找 n -= 1 if i &lt; n: nums[i] = nums[n] n -= 1 i += 1 else: i += 1 return i 26. 删除排序数组中的重复项给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例 1: 给定数组 nums = [1,1,2], 函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。 你不需要考虑数组中超出新长度后面的元素。示例 2: 给定 nums = [0,0,1,1,1,2,2,3,3,4], 函数应该返回新的长度 5, 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4。 你不需要考虑数组中超出新长度后面的元素。 思路见第80题 代码见第80题 80. 删除排序数组中的重复项 II给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素最多出现两次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例 1: 给定 nums = [1,1,1,2,2,3], 函数应返回新长度 length = 5, 并且原数组的前五个元素被修改为 1, 1, 2, 2, 3 。 你不需要考虑数组中超出新长度后面的元素。 思路快慢指针，慢指针从0位置开始，快指针遍历数组当快指针指向的数字等于慢指针左边第x个数字或者慢指针的位数小于x时快指针指向的数字赋值给慢指针指向的位置慢指针右移一位上述思路为解决此类问题的模板，x为保留的重复数字的个数所以第26的思路与此相同 代码12345678910111213class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ i = 0 for num in nums: if i &lt; 2 or nums[i-2] != num: nums[i] = num i +=1 return i]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode_Outline]]></title>
    <url>%2F2019%2F07%2F11%2FLeetCode-Outline%2F</url>
    <content type="text"><![CDATA[前言正式刷LeetCode啦，因为官方的题目只有分类而缺少题目之间的关联。有幸找到一位博主为LeetCode的题目做了二次分类整理，非常适合刷刷刷。这是该博主原文链接 索引下面是我刷题总结的博文索引包括题解分析和代码 Array 题目ID 题解 描述 基础 &nbsp; &nbsp; 27 移除元素 &nbsp; 26 删除排序数组中的重复项 &nbsp; 80 删除排序数组中的重复项 II &nbsp;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[object_detect]]></title>
    <url>%2F2018%2F11%2F13%2Fobject-detect%2F</url>
    <content type="text"><![CDATA[py-faster-rcnnFaster R-CNN是一种object detection算法，这里记录下Faster R-CNN的Caffe、python实现。git地址：https://github.com/rbgirshick/py-faster-rcnn 安装py-faster-rcnnClone the py-faster-rcnn就是从github把源码下载下来，由于网络等原因，切记核对源代码是否下载完整。 Makefile.config配置编译caffe所需的配置文件：文件内容说明：1、 将makefile.config里面的 WITH_PYTHON_LAYER :=1前面的注释去掉，这是因为faster R-CNN是要Python接口的，所以这一项要有，不能注释。2、 # Whatever else you find you need goes here.INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial此处其实就是加上了hdf5的路径3、 在编译caffe的Mkefile.config时遇到提示 nvcc warning : The ‘compute_20’, ‘sm_20’, and ‘sm_21’ architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning). 原因是，Makefile.config中采用了CUDA的compute capability 2.0和2.1，这是两种计算能力。从CUDA 8.0开始compute capability 2.0和2.1被弃用了，所以可以将-gencode arch=compute_20,code=sm_20 和-gencode arch=compute_20,code=sm_21这两行删除即可。4、 # CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cudacuda路径不可错，若机器上安装过两个cuda，此处配置应与~/.bashrc中配置的路径一致 将 Makefile.config复制到py-faster-rcnn/目录下 修改caffe-fast-rcnn但是由于代码提供者的caffe版本是老版本的，没有”与时俱进“，所以只兼容CUDNN 较老版本的。对于新的版本的，直接编译会报错。参考链接：https://blog.csdn.net/u010733679/article/details/52221404其中给出了两种方法：一种是到github去下载新版的caffe并将它与py-faster-rcnn中的caffe-faster-rcnn合并，更换成新版的caffe，使其支持新的cuDNN版本；另一种是自己手动更改一些文件，更改caffe的一些配置使其兼容新的cuDNN。第一种方法较简单，但是一旦工程被修改过就很容易出错，不推荐；所以我采用了第二种方法。1、用最新版本的caffe源码目录中的如下文件替换py-faster-rcnn/caffe-fast-rcnn中的对应文件。 include/caffe/layers/cudnn_relu_layer.hpp, src/caffe/layers/cudnn_relu_layer.cpp, src/caffe/layers/cudnn_relu_layer.cu include/caffe/layers/cudnn_sigmoid_layer.hpp, src/caffe/layers/cudnn_sigmoid_layer.cpp, src/caffe/layers/cudnn_sigmoid_layer.cu include/caffe/layers/cudnn_tanh_layer.hpp, src/caffe/layers/cudnn_tanh_layer.cpp, src/caffe/layers/cudnn_tanh_layer.cu 2、用caffe源码中的这个文件替换掉faster rcnn 对应文件 include/caffe/util/cudnn.hpp 3、打开 py-faster-rcnn/caffe-fast-rcnn 中的 src/caffe/layers/cudnn_conv_layer.cu文件，并将： cudnnConvolutionBackwardData_v3 替换为 cudnnConvolutionBackwardData cudnnConvolutionBackwardFilter_v3 替换为 cudnnConvolutionBackwardFilter 编译Cython modules进入目录：py-faster-rcnn/lib将Makefile文件中的python改为python2 因为此机器默认python版本为3.X在终端中运行“make” 开始编译 编译 Caffe and pycaffe打开py-faster-rcnn/caffe-fast-rcnn终端执行“make -j8 &amp;&amp; make pycaffe” 运行demo获取faster_rcnn_models参考https://blog.csdn.net/a8039974/article/details/77628805 获取imagenet_models参考https://blog.csdn.net/a8039974/article/details/77628805 将这两个文件夹复制到/home/ubuntu/lihaizhou/py-faster-rcnn/data 回到路径/home/ubuntu/lihaizhou/py-faster-rcnn/在终端执行“python2 tools/demo.py”如果运行正常会自动弹出标注成功的图片]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>object_detect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy]]></title>
    <url>%2F2018%2F08%2F02%2Fscrapy%2F</url>
    <content type="text"><![CDATA[简介Scrapy，Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 安装依赖： Python 2.7 Python Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。 lxml. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html OpenSSL. 除了Windows之外的系统都已经提供。 使用pip安装scrapypip install Scrapy 基本配置user_agent大多数情况下，网站都会根据我们的请求头信息来区分你是不是一个爬虫程序，如果一旦识别出这是一个爬虫程序，很容易就会拒绝我们的请求，因此我们需要给我们的爬虫手动添加请求头信息，来模拟浏览器的行为，但是当我们需要大量的爬取某一个网站的时候，一直使用同一个User-Agent显然也是不够的，因此，我们本节的内容就是学习在scrapy中设置随机的User-Agent。首先在settings.py文件中添加如下的信息。12345678910111213141516171819202122232425262728293031323334353637MY_USER_AGENT = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5", "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre", "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11", "Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36", ] 而后，在middlewares.py文件中添加如下的信息123456789101112131415161718192021import randomfrom scrapy import signalsfrom scrapy.downloadermiddlewares.useragent import UserAgentMiddlewareclass MyUserAgentMiddleware(UserAgentMiddleware): ''' 设置User-Agent ''' def __init__(self, user_agent): self.user_agent = user_agent @classmethod def from_crawler(cls, crawler): return cls( user_agent=crawler.settings.get('USER_AGENTS_LIST') ) def process_request(self, request, spider): agent = random.choice(self.user_agent) request.headers['User-Agent'] = agent 最后一步，就是将我们自定义的这个MyUserAgentMiddleware类添加到DOWNLOADER_MIDDLEWARES123DOWNLOADER_MIDDLEWARES = &#123; 'projectname.middlewares.MyUserAgentMiddleware': 400,&#125; hello xicihttp://www.xicidaili.com/每日更新免费HTTP代理。国内每个省的http匿名代理实时更新,可以根据你的需要分批提取。spider：xici.py12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-import scrapyfrom science.items import ScienceItemclass XiciSpider(scrapy.Spider): name = 'xici' allowed_domains = ['xicidaili.com'] start_urls = ['http://xicidaili.com/'] def start_requests(self): # 制作链接 reqs = [] for i in range(1, 10): req = scrapy.Request("http://www.xicidaili.com/nn/%s" % i) reqs.append(req) return reqs def parse(self, response): trs = response.xpath('//table/tr') items = [] for tr in trs[1:]: pre_item = ScienceItem() pre_item['IP'] = tr.xpath('td[2]/text()').extract_first() pre_item['PORT'] = tr.xpath('td[3]/text()').extract_first() pre_item['POSITION'] = tr.xpath('td[4]/a/text()').extract_first() pre_item['TYPE'] = tr.xpath('td[5]/text()').extract_first() pre_item['SPEED'] = tr.xpath('td[7]/div/@title').extract_first() pre_item['LAST_CHECK_TIME'] = tr.xpath('td[10]/text()').extract_first() items.append(pre_item) return items 数据存储：pipelines.py12345678910111213141516171819202122# -*- coding: utf-8 -*-import MySQLdbclass SciencePipeline(object): def process_item(self, item, spider): DBKWAGS = spider.settings.get('DBKWAGS') con = MySQLdb.connect(**DBKWAGS) cur = con.cursor() sql = ("insert into xici (IP, PORT,TYPE,POSITION ,SPEED,LAST_CHECK_TIME) " "value (%s,%s,%s,%s,%s,%s)") lis = (item['IP'], item['PORT'], item['TYPE'], item['POSITION'], item['SPEED'], item['LAST_CHECK_TIME']) try: cur.execute(sql, lis) except: print("Insert error") else: con.commit() cur.close() con.close() return item 设置文件：settings.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# -*- coding: utf-8 -*-BOT_NAME = 'science'SPIDER_MODULES = ['science.spiders']NEWSPIDER_MODULE = 'science.spiders'# 数据库参数配置DBKWAGS = &#123;'db': 'crawl', 'user': 'root', 'passwd': '', 'host': 'localhost', 'use_unicode': True, 'charset': 'utf8'&#125;# Obey robots.txt rulesROBOTSTXT_OBEY = False# Enable or disable downloader middlewares# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.htmlDOWNLOADER_MIDDLEWARES = &#123; 'science.middlewares.ScienceDownloaderMiddleware': 543, 'science.middlewares.MyUserAgentMiddleware': 400,&#125;# Configure item pipelines# See https://doc.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; 'science.pipelines.SciencePipeline': 300,&#125;USER_AGENTS_LIST = [ "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)", "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)", "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)", "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)", "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)", "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6", "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1", "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0", "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5", "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20", "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)", "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)", "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1", "Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5", "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre", "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11", "Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10"]]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raspbian_mysql]]></title>
    <url>%2F2018%2F07%2F31%2Fraspbian-mysql%2F</url>
    <content type="text"><![CDATA[安装mysql使用管理员权限运行apt-get获取最新的MySQL及Python编程接口（之后用于数据库编程）：1sudo apt-get install mysql-server python-mysqldb 安装如果失败则按照上篇博文raspbian_sources更换源安装成功后直接结束，并未提示输入数据库密码 密码设置参考：sudo mysql -u root -p select Host,User,plugin from mysql.user where User=’root’; plugin(加密方式)是unix_socket 运行如下三条命令：123update mysql.user set plugin='mysql_native_password'; #重置加密方式update mysql.user set password=PASSWORD("newpassword") where User='root'; #设置新密码flush privileges; #刷新权限信息 远程访问修改配置文件：/etc/mysql/mariadb.conf.d/50-server.cnf注释掉bind-address项 只有这些仍然不够，我们只是开启了MySQL监听远程连接的选项，接下来需要给对应的MySQL账户分配权限，允许使用该账户远程连接到MySQL 查看账号信息：select User, host from mysql.user; root账户中的host项是localhost表示该账号只能进行本地登录，我们需要修改权限，输入命令：1GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'password' WITH GRANT OPTION; 修改权限。%表示针对所有IP，password表示将用这个密码登录root用户，如果想只让某个IP段的主机连接，可以修改为1GRANT ALL PRIVILEGES ON *.* TO 'root'@'192.168.100.%' IDENTIFIED BY 'my-new-password' WITH GRANT OPTION; 注意：此时远程连接的密码可能与你在本地登录时的密码不同了，主要看你在IDENTIFIED BY后面给了什么密码最后别忘了FLUSH PRIVILEGES; 重启数据库sudo /etc/init.d/mysql restart 如果这些都做完了，还是不能连接，可以看一下端口是不是被防火墙拦截了]]></content>
      <categories>
        <category>raspbian</category>
      </categories>
      <tags>
        <tag>raspbian</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raspbian_sources]]></title>
    <url>%2F2018%2F07%2F31%2Fraspbian-sources%2F</url>
    <content type="text"><![CDATA[引用自：https://www.jianshu.com/p/67b9e6ebf8a0 树莓派基金会提供的源/etc/apt/sources.list.d/raspi.list里的软件源是树莓派基金会单独（非Raspbian开发者）提供/维护的软件源，主要包括raspi-config、minecraftpi、树莓派桌面环境、内核固件驱动等少量软件。这个软件源相关资料比较少，国内目前只有清华（201709开始提供）和中科大有提供 Stretch（三选其一即可）123456中科大deb https://mirrors.ustc.edu.cn/archive.raspberrypi.org/ stretch main ui清华deb https://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui默认官方源deb http://archive.raspberrypi.org/debian/ stretch main ui Jessie（三选其一即可）123456中科大deb https://mirrors.ustc.edu.cn/archive.raspberrypi.org/ jessie main ui清华deb https://mirrors.tuna.tsinghua.edu.cn/raspberrypi/ jessie main ui默认官方源deb http://archive.raspberrypi.org/debian/ jessie main ui Raspbian源raspbian这个源是由独立开发者维护的，与树莓派基金会并无直接联系。国内源比较多，选择一个与自己延迟最小或连接速度最好或物理距离最短的即可。无需担心各个软件源内容上会有不同，基本上各个站点每天都会同步一次，绝大部分情况下某软件安装不了与用了哪个站点提供的源无关。通常位于/etc/apt/sources.list Stretch（选择其中一个即可，注意大小写及目录）1234567891011121314151617181920212223242526272829303132中科大deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi 清华deb https://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi大连东软deb http://mirrors.neusoft.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi重庆大学deb http://mirrors.cqu.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi浙江大学deb http://mirrors.zju.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi阿里云deb http://mirrors.aliyun.com/raspbian/raspbian/ stretch main contrib non-free rpi 搜狐deb http://mirrors.sohu.com/raspbian/raspbian/ stretch main contrib non-free rpi 元智大学（中国台湾）deb http://ftp.cse.yzu.edu.tw/Linux/raspbian/raspbian/ stretch main contrib non-free rpi 新加坡国立大学deb http://mirror.nus.edu.sg/raspbian/raspbian/ stretch main contrib non-free rpi北陆先端科学技术大学院大学（日本知名镜像站，日常出口带宽2g）deb http://ftp.jaist.ac.jp/raspbian/ stretch main contrib non-free rpi牛津大学deb http://mirror.ox.ac.uk/sites/archive.raspbian.org/archive/raspbian/ stretch main contrib non-free rpi美国Berkely大学deb http://mirrors.ocf.berkeley.edu/raspbian/raspbian/ stretch main contrib non-free rpi美国俄克拉荷马大学deb http://reflection.oss.ou.edu/raspbian/raspbian/ stretch main contrib non-free rpi南非知名软件源deb http://mirror.liquidtelecom.com/raspbian/raspbian/ stretch main contrib non-free rpi 默认源（带重定向by mirrorbrain）deb http://mirrordirector.raspbian.org/raspbian/ stretch main contrib non-free rpi官方源deb https://archive.raspbian.org/raspbian/ stretch main contrib non-free rpi Jessie（选择其中一个即可，注意大小写及目录）1234567891011121314151617181920212223242526272829303132中科大deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpi 清华deb https://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpi大连东软deb http://mirrors.neusoft.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpi重庆大学deb http://mirrors.cqu.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpi浙江大学deb http://mirrors.zju.edu.cn/raspbian/raspbian/ jessie main contrib non-free rpi阿里云deb http://mirrors.aliyun.com/raspbian/raspbian/ jessie main contrib non-free rpi 搜狐deb http://mirrors.sohu.com/raspbian/raspbian/ jessie main contrib non-free rpi 元智大学（中国台湾）deb http://ftp.cse.yzu.edu.tw/Linux/raspbian/raspbian/ jessie main contrib non-free rpi 新加坡国立大学deb http://mirror.nus.edu.sg/raspbian/raspbian/ jessie main contrib non-free rpi北陆先端科学技术大学院大学（日本知名镜像站，日常出口带宽2g）deb http://ftp.jaist.ac.jp/raspbian/ jessie main contrib non-free rpi 牛津大学deb http://mirror.ox.ac.uk/sites/archive.raspbian.org/archive/raspbian/ jessie main contrib non-free rpi 美国Berkely大学deb http://mirrors.ocf.berkeley.edu/raspbian/raspbian/ jessie main contrib non-free rpi美国俄克拉荷马大学deb http://reflection.oss.ou.edu/raspbian/raspbian/ jessie main contrib non-free rpi南非知名软件源deb http://mirror.liquidtelecom.com/raspbian/raspbian/ jessie main contrib non-free rpi 默认源（带重定向by mirrorbrain）deb http://mirrordirector.raspbian.org/raspbian/ jessie main contrib non-free rpi官方源deb https://archive.raspbian.org/raspbian/ jessie main contrib non-free rpi]]></content>
      <categories>
        <category>raspbian</category>
      </categories>
      <tags>
        <tag>raspbian</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx]]></title>
    <url>%2F2018%2F07%2F31%2Fnginx%2F</url>
    <content type="text"><![CDATA[tornadoTornado是一种Web服务器软件的开源版本。Tornado和现在的主流Web服务器框架（包括大多数Python的框架）有着明显的区别：它是非阻塞式服务器，而且速度相当快。得利于其非阻塞的方式和对epoll的运用，Tornado每秒可以处理数以千计的连接，因此Tornado是实时Web服务的一个理想框架。pip install tarnado hello tornado最小应用：123456789101112131415161718192021222324252627import tornado.httpserverimport tornado.ioloopimport tornado.optionsimport tornado.webfrom tornado.options import define, optionsdefine("port", default=8001, help="run on the given port", type=int)class MainHandler(tornado.web.RequestHandler): def get(self): self.write("Hello, Tornado")def main(): tornado.options.parse_command_line() application = tornado.web.Application([ (r"/", MainHandler), ]) http_server = tornado.httpserver.HTTPServer(application) http_server.listen(options.port) tornado.ioloop.IOLoop.instance().start()if __name__ == "__main__": main() nginxNginx是一个高性能的HTTP和反向代理服务器，其特点是占有内存少，并发能力强。反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。安全反向代理有许多用途： 可以提供从防火墙外部代理服务器到防火墙内部安全内容服务器的加密连接。 可以允许客户机安全地连接到代理服务器，从而有利于安全地传输信息（如信用卡号）。 安全反向代理会造成各安全连接因加密数据所涉及的系统开销而变慢。但是，由于 SSL 提供了高速缓存机制，所以连接双方可以重复使用先前协商的安全参数，从而大大降低后续连接的系统开销。 hello nginx配置文件放在 /etc/nginx/sites-enabled本来是放在 /etc/nginx/sites-available 然后用软链接链到/etc/nginx/sites-enabled但我不知怎么链过去文件总是空白的，所以简单粗暴复制过去。重启nginxsudo /etc/init.d/nginx restart nginx 代理 tornado在/etc/nginx/sites-enabled创建配置文件12345678910111213141516171819202122232425# this is the real tornado service upstream tornado &#123; server 127.0.0.1:8001; &#125; server &#123; listen 8000; root /var/www/tornado/; index app.py; server_name server; location / &#123; if (!-e $request_filename) &#123; rewrite ^/(.*)$ /center.py/$1 last; &#125; &#125; location ~ / &#123; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://tornado; &#125; &#125; 所有 http://localhost:8000 的请求都会转发到upstream tornado{}里面列出的服务里面。 supervisor安装sudo apt-get install supervisor安装成功后,supervisor就会默认启动将每个进程的配置文件单独拆分,放在/etc/supervisor/conf.d/目录下,以.conf作为扩展名配置文件内容：12[program:tornado_hello]command=python /var/www/tornado/hello.py 重启supervisor,让配置文件生效,然后启动test进程:12supervisorctl updatesupervisorctl]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe-deploy]]></title>
    <url>%2F2018%2F07%2F30%2Fcaffe-deploy%2F</url>
    <content type="text"><![CDATA[简介Caffe，全称Convolutional Architecture for Fast Feature Embedding。是一种常用的深度学习框架，主要应用在视频、图像处理方面的应用上。此次主要唠唠关于它的部署 安装依赖123456sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libatlas-base-devsudo apt-get install libhdf5-serial-devsudo apt-get install python-devsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev 在caffe-master/python下：1for req in $(cat requirements.txt); do pip install $req; done 下载Caffegit clone https://github.com/BVLC/caffe.git贼卡，可以去github用其他工具下载。解压：unzip -n caffe-master.zip -d ./ 安装caffe12cd caffecp Makefile.config.example Makefile.config 在Makefile.config文件中把CPU_ONLY := 1的注释给去掉修改如下:12INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial 计算能力 mkl &gt; openlas &gt;atlas，为了简便，默认atlas]]></content>
      <categories>
        <category>caffe</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim]]></title>
    <url>%2F2018%2F05%2F29%2Fvim%2F</url>
    <content type="text"><![CDATA[基础操作 命令 操作 记忆方法 :q 退出 quit :w 存盘 write :e 打开新文件 :r 读取文件 read :! 强行 :set nu 显示行号 number :set nonu 隐藏行号 no number Ctrl + f 翻到下一页（向前翻页） front Ctrl + b 翻到上一页（向后翻页） back Ctrl + u 向前翻半页 Ctrl + d 向后翻半页 ^ 移到行头 往上就到行头了 $ 移到行尾 写完一行就要给一行的钱 w 下一个单词 word b 前一个单词 behind（在。。。后面） e 下一单词尾 end #G 跳到某一行 大哥(G)说到哪就到哪 i 光标前插入 insert a 光标后加入 add A 在行末加入 在一个词后是小a,一个行后就是大A o 另起一行加入 一个小鸡蛋（小o）掉下来了摔开了花 O 上一行加入 吐一个大泡泡（大O）飞上去破了 行内删除 命令 操作 记忆方法 cw 删除一个单词（一部分不包括空格） 吃掉一个 word c$ 删除一行到行尾 刚写的一行被删了，钱也拿不到了 c^ 删除一行到行头 往上吃，一直吃到头 x 删除一个字符 看你不爽就打上“x” 删除操作 命令 操作 记忆方法 dd 删除一行 del dir dw 删除单词到尾部（包括空格） del word de 删除单词到尾部（不包括尾部空格） del end d$ 删除当前到行尾的所有字符 del $(代表尾部) d^ 删除当前到行首的所有字符 del ^(代表行首) J 合并当前行 一个大钩子(J)把下面的一行拉到自己行尾 u 撤销上次操作 undo U 撤销当前行所有操作 事情闹大了，得有个更大的UNDO才能恢复 Ctrl + r 恢复undo recover 复制粘贴 命令 操作 记忆方法 yy 复制当前行整行的内容到缓冲区 p 读取缓冲区中的内容，并粘贴到光标当前的位置 查找 命令 操作 记忆方法 /word 从上而下查 /是从上而下写的吧 ?word 从下而上查找 字符在哪儿呢（？）回头找找吧 n 定位下一个匹配的 相当于向下查找下一个 next N 定位上一个匹配的 相当于向上查找上一个 替换 命令 操作 记忆方法 :s/1/2 搜索当前行第一个1并用2代替 search :s/1/2/g 搜索当前行所有的1并用2代替 global :#,#s/1/2/g 在#,#间搜索所有1并用2替换 :%s/1/2/g 在整个文档中将1替换为2 100％（全部） :s/1/2/c 每次替换都给出提示确认 cue提示 r 取代光标所在处的字符 replace R 取代字符直到按Esc为止 强力replace 多文件 命令 操作 记忆方法 :args 显示多文件信息(会在末行提示当前打开了哪些档) are globals :next 切换到下一个文件 :prev 切换到上一个文件 :first 定位首文件 :last 定位尾文件 Ctrl + ^ 快速切换到编辑器中切换前的文件 光标移动 命令 操作 记忆方法 ) 光标移至句尾 } 光标移至段落开头 n+ 光标下移n行 n- 光标上移n行 n$ 光标移至第n行尾 H 光标移至屏幕顶行 M 光标移至屏幕中间行 L 光标移至屏幕最后行 0（数字） 光标移至当前行首 $ 光标移至当前行尾 nz 将第n行滚至屏幕顶部，不指定n时将当前行滚至屏幕顶部]]></content>
      <categories>
        <category>工具积累</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Raspberry Pi]]></title>
    <url>%2F2018%2F05%2F22%2Fpipipi%2F</url>
    <content type="text"><![CDATA[以下部分是以CentOs为例，但发现使用不方便，故丢在引用里，以Raspbain为例重写。 ##### 系统烧录适用于树莓派的系统列表这次用CentOs做例子讲解CentOs系统镜像列表下载下来的后缀为.raw.xz的文件要解压成.raw然后用Win32DiskImager工具烧录到闪存卡将闪存卡插入树莓派，通电，连接网线。##### 局域网连接树莓派算是做好了，如何食用呢？###### 有路由器将电脑和树莓派用网线连接到同一路由器1. 如果有路由器访问权限，直接查看树莓派的IP地址2. 用ipscaner工具可以查看同意局域网内的设备IP地址###### 无路由器电脑联网并将网络设置为共享模式树莓派用网线连接电脑（可用网线转接器）cmd输入命令arp -a在接口197.168.137.1下面的第一个IP地址即为电脑共享给树莓派的IP获得了IP就好说话了，用putty也好cmder也好，只要支持ssh访问的都行命令ssh username@ip即可连接##### 初始化配置###### SD卡扩展到实际容量在终端中输入fdisk /dev/mmcblk0进入硬盘分区软件在软件中输入：p——查看旧分区情况d——删除分区，并按照提示删除第三个分区n——添加一个分区，空间起始位置按照系统默认（默认是最大空间）p——查看新分区情况w——写入分区信息并退出软件在终端中输入：sudo reboot重启树莓派在树莓派开机后在终端输入： resize2fs /dev/mmcblk0p3重新加载分区信息使用：df -h查看新的分区信息表###### 设置网络配置信息123nmcli d wifi #查看周围的wifinmcli d wifi connect yourSSID password 'yourpassword' #连接wifi nmcli d show wlan0 #查看wlan0的状态接下来设置网络配置信息123456vi etc/sysconfig/network-scripts/ifcfg-0000 #0000是wifi的名字BOOTPROTO=static #静态IPIPADDR=192.168.0.160 #IP地址GATEWAY=192.168.1.1 #默认网关NETMASK=255.255.255.0 #子网掩码###### 创建新用户新建用户并初始化密码：12adduser yourusernamepasswd yourusernamelinux会判断密码复杂度，不过可以强行忽略给新建用户授权：新创建的用户并不能使用sudo命令，需要给他添加授权。sudo命令的授权管理是在sudoers文件里的12[root@localhost /]# ls -l /etc/sudoers-r--r-----. 1 root root 3938 Apr 10 20:27 /etc/sudoers只有只读的权限，如果想要修改的话，需要先添加w权限12[root@localhost /]# chmod -v u+w /etc/sudoersmode of ‘/etc/sudoers’ changed from 0440 (r--r-----) to 0640 (rw-r-----)然后就可以添加内容了1[root@localhost /]# vi /etc/sudoers在下面的一行下追加新增的用户:123## Allow root to run any commands anywher root ALL=(ALL) ALL Hodge ALL=(ALL) ALL #这个是新增的用户记得将写权限收回:12[root@localhost /]# chmod -v u-w /etc/sudoersmode of ‘/etc/sudoers’ changed from 0640 (rw-r-----) to 0440 (r--r-----)###### 更换阿里源1. 备份mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup2. 下载新的CentOS-Base.repo 到/etc/yum.repos.d/wget http://mirrors.aliyun.com/repo/Centos-altarch-7.repo -O /etc/yum.repos.d/CentOS-Base.repo3. 初始化12[root@localhost yum.repos.d]# yum clean all[root@localhost yum.repos.d]# yum makecache 系统烧录适用于树莓派的系统列表这次用Raspbain做例子讲解Raspbain系统下载下载下来的后缀为.zip的文件要解压成.img然后用Win32DiskImager工具烧录到闪存卡将闪存卡插入树莓派，通电，连接网线。 局域网连接树莓派算是做好了，如何食用呢？ 有路由器将电脑和树莓派用网线连接到同一路由器 如果有路由器访问权限，直接查看树莓派的IP地址 用ipscaner工具可以查看同意局域网内的设备IP地址 无路由器电脑联网并将网络设置为共享模式树莓派用网线连接电脑（可用网线转接器）cmd输入命令arp -a在接口197.168.137.1下面的第一个IP地址即为电脑共享给树莓派的IP可用ping命令测试 获得了IP就好说话了，用putty也好cmder也好，只要支持ssh访问的都行命令ssh username@ip即可连接 关于ssh获取树莓派IP后用ssh工具连接时可能出现连接被拒绝等报错可参考：树莓派新系统SSH连接被拒绝的解决方法 初始化配置修改密码sudo passwd pi 开启root用户由于root用户默认密码为空，因此无法登陆。登陆默认用户pi给root设置密码后即可开启root用户可使用su - root切换用户 SD卡扩展到实际容量使用：df -h查看新的分区信息表最新的Raspbain系统已自动充分利用闪存卡空间了，无需额外设置。 设置网络配置信息连接无线网： 用户可以在未启动树莓派的状态下单独修改 /boot/wpa_supplicant.conf 文件（内存卡插电脑）配置 WiFi 的 SSID 和密码，这样树莓派启动后会自行读取 wpa_supplicant.conf 配置文件连接 WiFi 设备。 树莓派启动状态下使用sudo nano /etc/wpa_supplicant/wpa_supplicant.conf打开该文件 编辑内容如下：123456789101112131415161718country=CNctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1 network=&#123;ssid="WiFi-A"psk="12345678"key_mgmt=WPA-PSKpriority=1&#125; network=&#123;ssid="WiFi-B"psk="12345678"key_mgmt=WPA-PSKpriority=2scan_ssid=1&#125; ssid:网络的ssidpsk:密码priority:连接优先级，数字越大优先级越高（不可以是负数）scan_ssid:连接隐藏WiFi时需要指定该值为1如果你的 WiFi 没有密码或使用WEP加密key_mgmt=NONE如果你的 WiFi 使用WPA/WPA2加密key_mgmt=WPA-PSK 配置静态IP：修改/etc/dhcpcd.conf 文件1234567891011interface eth0static ip_address=192.168.0.10/24static routers=192.168.0.1static domain_name_servers=192.168.0.1interface wlan0static ip_address=192.168.0.200/24static routers=192.168.0.1static domain_name_servers=192.168.0.1 上面的配置文件中 , eth0是有线的配置 , wlan0是无线配置ip_address就是静态IP , 后面要接/24routers是网关static domain_name_servers是DNS 更换软件源选取中科大的软件源 备份mv /etc/apt/sources.list /etc/apt/sources.list.backup 新建sources.list并写入如下内容 12deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ jessie main non-free contribdeb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ jessie main non-free contrib 更新软件列表 1sudo apt-get update 架设服务器 安装Apache2 1sudo apt-get install apache2 安装sqlite 1sudo apt-get install sqlite 远程访问远程桌面局域网内windows远程桌面连接树莓派 必须先安装tightvncserver！！！ 1sudo apt-get install tightvncserver 再安装xrdp服务。 1sudo apt-get install xrdp 打开Windows的远程桌面功能再输入目标局域网IP即可连接。 nat123环境安装：123456#apt-get update //更新，新系统推荐运行#apt-get install mono-complete //安装运行环境，一路YES下去#cd /mnt // mnt为你想要安装nat123的目录#wget http://www.nat123.com/down/nat123linux.tar.gz //下载软件# tar -zxvf nat123linux.tar.gz //解压#mono nat123linux.sh //运行 运行并按提示依次输入自己的帐号和密码、运行成功当然你不想这个程序一直占用你的屏幕，接下来输入x 回车退出软件我们要客户端后台服务启动，这样才清爽：12#cd /mnt //将mnt换成是自己本地实际安装目录#mono nat123linux.sh service &amp; 运行显示 nat123 service is running…. 就ok啦 按下回车键 就可以将其后台运行了如果你还是不放心，可查看其运行情况：ps -ef|grep nat12312pi 23599 23577 5 02:45 pts/0 00:00:02 mono nat123linux.sh servicepi 23688 23577 0 02:46 pts/0 00:00:00 grep --color=auto nat123 至此树莓派raspbian/Linux上软件的配置与运行登录完成~~~~ 终于到了重头戏了端口映射配置登录到 http://www.nat123.com/UsersNatItem.jsp在映射编辑页面：应用类型：因为全端口映射需要安装nat123的软件才可以访问内网，故不再折腾重点讲80网站类型和其他（非网站类型） 80网站类型、映射线路默认、应用名称（随你咯）、内网端口为80、内网地址为树莓派静态IP地址、外网域名确认保存后在自己的域名解析管理网站将DNS服务器或NS设置为dns1.dns123.net/dns3.dns123.net；或者将cname设置为指定值 其他（非网站类型）一般用来做ssh连接或远程桌面连接映射线路默认、应用名称（随你咯）、内网端口ssh为22，xrdp为3350、内网地址为树莓派静态IP地址、外网端口按要求设置、外网域名确认保存后在自己的域名解析管理网站将DNS服务器或NS设置为dns1.dns123.net/dns3.dns123.net；或者将cname设置为指定值 nat123自启动要保证在断网、断电、重启后nat123服务自动启动： 编写autonat123.sh脚本放在/mnt目录下： 12345678910cd /home/pi/Public/nat123#echo "1" &gt; times.logps -fe|grep nat123linux.sh |grep -v grepif [ $? -ne 0 ];thenvar=`date +%Y/%m/%d-%H:%M:%S`echo $var &gt;&gt; runtimes.logmono nat123linux.sh autologin username passwd#else#echo ".....nat123linux.....is.........runing......................."fi 通过crontab -e设置自动任务，每10小时运行autonat123.sh将以下指令加入文件最后0 */10 * * * cd /mnt &amp;&amp; sh autonat123.sh 将autonat123.sh脚本加入开机自启动：1sudo vi /etc/rc.local 加入启动指令sh /mnt/autonat123.sh]]></content>
      <categories>
        <category>Raspberry Pi</category>
      </categories>
      <tags>
        <tag>Raspberry Pi</tag>
        <tag>CentOs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[起步与KNN]]></title>
    <url>%2F2018%2F05%2F18%2Fcs231n%2F</url>
    <content type="text"><![CDATA[基于卷积神经网络（CNN）的视觉识别 挑战 视觉点变化，观察的视角差异 规模尺度变化,巨人与小恶魔 内容可能是形态可变化的，扭曲的猫 观察物被遮挡 光照条件影响及其大 背景图像的影响 类型内的影响，比如‘主持人’这一分类范围较广 图片分类流程： 输入：N张图片组成的数据集，每张图片有一个明确的label 学习：利用数据集去学习每个label的特征，称之为“训练一个分类器”或“学习一个模型” 评估：用新的数据集去测试训练好的分类器，计算准确率 Nearest Neighbor Classifier最近邻分类器和CNN无关、也极少用于实践，但能帮我们了解图片分类问题思路：对于训练集，读取成为长*宽*3的矩阵。对于测试集，与前面训练集中的图片比较，找出与其最相似的图片则他们分为一类图片比较方法：1、计算两矩阵中对应两两位置的差值的和2、计算两矩阵中对应两两位置的差值的平方和然后再开根号1Evaluate on the test set only a single time, at the very end. 1将您的训练集分成训练集和验证集。使用验证集来调整所有超参数。最后在测试集上运行一次并报告性能。 Nearest Neighbor Classifier 的优缺点：优点：简单易懂，低训练成本。缺点：测试成本过高。逐像素距离根本不对应于感知或语义相似性。 交叉验证在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。这个过程一直进行，直到所有的样本都被预报了一次而且仅被预报一次。把每个样本的预报误差平方加和，称为PRESS(predicted Error Sum of Squares)。]]></content>
      <categories>
        <category>CS231n</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Web]]></title>
    <url>%2F2018%2F05%2F18%2Fpyweb%2F</url>
    <content type="text"></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opensuse安装]]></title>
    <url>%2F2018%2F05%2F08%2Fopensuse%2F</url>
    <content type="text"><![CDATA[安装官网下载leap版下载U盘刻录工具Rufus 神器，不多提。制作好启动盘分区方案选专家模式，自定义。安装结束后进入windows的磁盘管理器把opensuse对应的几个磁盘的卷标删除即可。 配置]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>双系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git_to_all]]></title>
    <url>%2F2018%2F05%2F06%2Fgit-to-all%2F</url>
    <content type="text"><![CDATA[个人仓库管理初始化操作因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。12$ git config --global user.name "Your Name"$ git config --global user.email "email@example.com" 新建文件夹并创建为本地git仓库1234$ mkdir gittest$ cd gittest\$ git initInitialized empty Git repository in I:/gittest/.git/ 可见bash提示你创建了一个空的git仓库 修改与提交创建readme.txt，内容如下123这是一个测试文件hello git 把文件添加到git仓库暂存区（stage）1$ git add readme.txt git add. ：会把工作时的所有变化提交到暂存区，包括文件内容修改(modified)以及新文件(new)，但不包括被删除的文件。git add -u ：仅会将被修改的文件提交到暂存区。add -u 不会提交新文件。（git add –update的缩写）git add -A ：是上面两个功能的合集（git add –all的缩写）提交变更到当前分支并加上说明1234$ git commit -m "add readme"[master (root-commit) e5208f9] add readme 1 file changed, 3 insertions(+) create mode 100644 readme.txt 可见bash提示提交变更到master分支成功 仓库当前的状态修改reademe.txt内容为如下1234这是一个测试文件这是新增的一行hello gitadd a row 使用命令查看仓库状态12345678910$ git add readme.txt$ git statusOn branch masterChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: readme.txtno changes added to commit (use "git add" and/or "git commit -a") git diff file_name可以查看文件具体修改的内容，不过显示的格式是Unix通用的diff格式，看不太清楚查看提交历史123456789101112$ git log commit 69f64026b1537fdb1624bd0dd58080e0a40fde7c (HEAD -&gt; master) Author: Hodge &lt;2991811241@qq.com&gt; Date: Sun May 6 19:09:09 2018 +0800 第二次提交：新增两行 commit e5208f91bf4f875e22c4fdf6ce53d02169cb1427 Author: Hodge &lt;2991811241@qq.com&gt; Date: Sun May 6 18:54:06 2018 +0800 add readme 后悔药查看提交历史精简版1234$ git log --pretty=oneline7fdbfe57223ebfce5a34580213d11bd3e988a354 (HEAD -&gt; master) 第三次修改69f64026b1537fdb1624bd0dd58080e0a40fde7c 第二次提交：新增两行e5208f91bf4f875e22c4fdf6ce53d02169cb1427 add readme 回退到某一版本git reset --hard 版本id(只写少数几位即可)12$ git reset --hard 69f6402HEAD is now at 69f6402 第二次提交：新增两行 此时的readme.txt内容如下1234这是一个测试文件这是新增的一行hello gitadd a row 时空穿越成功可是此时的git log如下123456789101112$ git logcommit 69f64026b1537fdb1624bd0dd58080e0a40fde7c (HEAD -&gt; master)Author: Hodge &lt;2991811241@qq.com&gt;Date: Sun May 6 19:09:09 2018 +0800 第二次提交：新增两行commit e5208f91bf4f875e22c4fdf6ce53d02169cb1427Author: Hodge &lt;2991811241@qq.com&gt;Date: Sun May 6 18:54:06 2018 +0800 add readme 第三次提交的记录不见了，看不到commit id如何恢复？12345$ git reflog69f6402 (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to 69f64027fdbfe5 HEAD@&#123;1&#125;: commit: 第三次修改69f6402 (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: 第二次提交：新增两行e5208f9 HEAD@&#123;3&#125;: commit (initial): add readme 不能再清晰。 撤销修改git checkout -- file_name命令git checkout – readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况：一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 撤销暂存区内容：Git同样告诉我们，用命令git reset HEAD file_name可以把暂存区的修改撤销掉（unstage），重新放回工作区： 删除文件Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了现在你有两个选择：一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本：git checkout -- file_name 远程仓库第1步：创建SSH Key1$ ssh-keygen -t rsa -C "youremail@example.com" 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。将公钥添加到github或码云~~~~ 在码云创建一个空的仓库命名为learngit链接远程仓库：1$ git remote add origin git@gitee.com:Hodge/learngit.git 第一次推送12345678910$ git push -u origin masterCounting objects: 6, done.Delta compression using up to 4 threads.Compressing objects: 100% (3/3), done.Writing objects: 100% (6/6), 555 bytes | 555.00 KiB/s, done.Total 6 (delta 0), reused 0 (delta 0)remote: Powered by Gitee.comTo gitee.com:Hodge/learngit.git * [new branch] master -&gt; masterBranch master set up to track remote branch master from origin. 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送到远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。从现在起，只要本地作了提交，就可以通过命令1$ git push origin master 把本地master分支的最新修改推送至码云，现在，你就拥有了真正的分布式版本库！ 分支管理廖雪峰大神的博客关于分支的动画大赞 实战首先，我们创建dev分支，然后切换到dev分支12$ git checkout -b devSwitched to a new branch 'dev' git checkout命令加上-b参数表示创建并切换，相当于以下两条命令123$ git branch dev$ git checkout devSwitched to branch 'dev' 然后，用git branch命令查看当前分支123$ git branch* dev master 此时是在dev分支上开发，修改reademe.txt文件内容如下：123456这是一个测试文件这是新增的一行hello gitadd a row这是在dev分支上的更改 切换回master分支注意！！此时readme.txt的内容变为：1234这是一个测试文件这是新增的一行hello gitadd a row 现在，我们把dev分支的工作成果合并到master分支上12345$ git merge devUpdating 69f6402..73c4f3fFast-forward readme.txt | 4 +++- 1 file changed, 3 insertions(+), 1 deletion(-) ok,现在readme.txt内容变为和dev分支一样，此时可以删除dev分支12$ git branch -d devDeleted branch dev (was 73c4f3f). 解决冲突请看如下操作：新建dev分支将readme.txt文件改为：1text:dev 提交后返回master分支在master分支将readme.txt文件改为：1text:master 提交后试图合并dev分支：1234$ git merge devAuto-merging readme.txtCONFLICT (content): Merge conflict in readme.txtAutomatic merge failed; fix conflicts and then commit the result. 因为在两个分支都对同一文件的同一位置进行了修改，git无法进行自动merge，提示需要手动解决冲突并且！！！在merge失败后readme.txt内容变成如下：12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADtext:master=======text:dev&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev 手动修改文件内容为：1text:master merge dev 提交后用带参数的git log也可以看到分支的合并情况123456789$ git log --graph --pretty=oneline --abbrev-commit* 372f62b (HEAD -&gt; master) 手动merge master and dev|\| * 7fc1214 (dev) text:dev* | bdfdaaf text:master|/* 73c4f3f 在dev分支添加一行* 69f6402 (origin/master) 第二次提交：新增两行* e5208f9 add readme 分支管理策略通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息准备合并dev分支，请注意–no-ff参数，表示禁用Fast forward1$ git merge --no-ff -m "merge with no-ff" dev 因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。合并后，我们用git log看看分支历史123456$ git log --graph --pretty=oneline --abbrev-commit* 7825a50 merge with no-ff|\| * 6224937 add merge|/* 59bc1cb conflict fixed Bug分支当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交1234567891011$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: readme.txtno changes added to commit (use "git add" and/or "git commit -a") readme.txt内容如下：123456789101111112222223333334444445555556666667dev77888888dev-------dev 并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作12$ git stashSaved working directory and index state WIP on master: 1c1e7ac Merge branch 'dev' 此时12345$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.nothing to commit, working tree clean readme.txt内容还原为如下：12345678111111master3333334444445555556666667dev77888888 新建bug-1分支，新增bug-1.txt,提交、回答master分支，merge bug-1分支，删除bug-1分支回到dev分支，工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看12$ git stash liststash@&#123;0&#125;: WIP on master: 1c1e7ac Merge branch 'dev' 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法：一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除；另一种方式是用git stash pop，恢复的同时把stash内容也删了1234567891011$ git stash popAuto-merging readme.txtOn branch devChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: readme.txtno changes added to commit (use "git add" and/or "git commit -a")Dropped refs/stash@&#123;0&#125; (530fcb79063bbcdeac0b414547158ef130bdca7e) 修改、提交、merge、删除分支、推送到远程仓库 Feature分支添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。但如果feature分支功能因为某些原因还没merge就需要删除，执行git branch -d 分支名时会销毁失败。Git友情提醒，feature分支还没有被合并，如果删除，将丢失掉修改，如果要强行删除，需要使用命令git branch -D 分支名。 多人协作当多人对同一个远程仓库具有操作权限时：当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认名称是origin多人协作的工作模式通常是这样： 首先，可以试图用git push origin branch-name推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream branch-name origin/branch-name。 Fork这种情况是：你不拥有某仓库的push权限，但你可以Fork别人的项目Fork后会在你的github上生成一个一样的仓库，你可以将其与自建仓库等同看待。你在这个仓库上开发，增加了一个很cool的功能，想提交到别人仓库，但是pull request 时可能会出错，因为你fork的repo和现在的别人的repo已经不一样了，因为别人也在开发并push啊。所以你要pull别人仓库最新的代码，然后手动merge冲突，然后再pull request并祈祷别人接受你的pull 标签管理tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。命令git tag &lt;name&gt;就可以打一个新标签，git tag查看所有标签。默认标签是打在最新提交的commit上的。对于以前提交的commit想要增加标签只需在添加标签语句后面添加commit id即可。可以用git show &lt;tagname&gt;查看标签信息。还可以创建带有说明的标签，用-a指定标签名，-m指定说明文字：1git tag -a v0.1 -m "version 0.1 released" 3628164 git tag -d v0.1删除标签将标签推送到远程：单个推送：git push origin v1.0全部推送git push origin --tags如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除、然后，从远程删除。删除命令也是push，但是格式如下：git push origin :refs/tags/&lt;tagname&gt; 自定义Git.gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理https://github.com/github/gitignore强制添加：git add -f App.class或者你发现，可能是.gitignore写得有问题，需要找出来到底哪个规则写错了，可以用git check-ignore -v &lt;filename&gt;命令检查。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[heapsort]]></title>
    <url>%2F2018%2F03%2F28%2Fheapsort%2F</url>
    <content type="text"><![CDATA[基础知识 普通的队列是一种先进先出的数据结构，元素在队列尾追加，而从队列头删除。在优先队列中，元素被赋予优先级。当访问元素时，具有最高优先级的元素最先删除。 堆（英语：heap)是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵树的数组对象。堆总是满足下列性质：1、堆中某个节点的值总是不大于或不小于其父节点的值；2、堆总是一棵完全二叉树。 用堆来实现优先队列最大元素在根节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Description: 基于堆的优先队列 * Author: Hodge * Date: 2018-03-28 */public class MaxPQ&lt;Key extends Comparable&lt;Key&gt;&gt;&#123; private Key[] pq; //基于堆的完全二叉树 private int N = 0; public MaxPQ(int manN) &#123; pq = (Key[]) new Comparable[manN + 1]; &#125; public boolean isEmpty() &#123; return N == 0; &#125; public int size() &#123; return N; &#125; public void insert(Key v) &#123; pq[++N] = v; swim(N); &#125; public Key delMax() &#123; Key max = pq[1]; //根节点为最大元素 exch(1, N--); //交换 pq[N + 1] = null; //防止对象游离 sink(1); //恢复堆的有序性 return max; &#125; private void sink(int k) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(j, j + 1)) &#123; j++; &#125; exch(k, j); k = j; &#125; &#125; private void swim(int k) &#123; while (k &gt; 1 &amp;&amp; less(k / 2, k)) &#123; exch(k / 2, k); k /= 2; &#125; &#125; private boolean less(int i, int j) //比较两个元素大小 &#123; return pq[i].compareTo(pq[j]) &lt; 0; &#125; private void exch(int i, int j) //交换两个元素位置 &#123; Key t = pq[i]; pq[i] = pq[j]; pq[j] = t; &#125;&#125; 堆排序将所有元素插入一个查找最小（或最大）元素的优先队列，通过重复调用删除最小元素的操作排序 堆的构造最傻方法：从左到右遍历数组，逐一使用swim()方法，复杂度NlogN优化：从N/2到1节点逐一使用sink()方法递归建立起堆的秩序 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import edu.princeton.cs.algs4.StdOut;import java.util.Scanner;/** * Description: 堆排序 * Author: Hodge * Date: 2018-03-28 */public class HeapSort&#123; private HeapSort() &#123; &#125; public static void sort(Comparable[] pq) &#123; int n = pq.length; for (int k = n / 2; k &gt;= 1; k--) //构造堆 sink(pq, k, n); while (n &gt; 1) //逐一取值 &#123; exch(pq, 1, n--); sink(pq, 1, n); &#125; &#125; private static void exch(Object[] pq, int i, int j) &#123; Object swap = pq[i - 1]; pq[i - 1] = pq[j - 1]; pq[j - 1] = swap; &#125; private static void sink(Comparable[] pq, int k, int n) &#123; while (2 * k &lt;= n) &#123; int j = 2 * k; if (j &lt; n &amp;&amp; less(pq, j, j + 1)) &#123; j++; &#125; if (!less(pq, k, j)) &#123; break; &#125; exch(pq, k, j); k = j; &#125; &#125; private static boolean less(Comparable[] pq, int i, int j) &#123; return pq[i - 1].compareTo(pq[j - 1]) &lt; 0; &#125; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); String input = sc.nextLine(); String arr[] = input.split(" "); Comparable[] num = new Comparable[arr.length]; for (int i = 0; i &lt; arr.length; i++) &#123; num[i] = Integer.parseInt(arr[i]); &#125; HeapSort.sort(num); show(num); &#125; private static void show(Comparable[] a) //打印数组 &#123; for (Comparable anA : a) &#123; StdOut.print(anA + " "); &#125; StdOut.println(); &#125;&#125; 分析将N个元素排序，堆排序只需要少于(2NlgN+2N)次比较以及一半次数的交换]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crawler]]></title>
    <url>%2F2018%2F03%2F22%2Fcrawler%2F</url>
    <content type="text"><![CDATA[简介爬虫可分为几大模块： URL管理器：管理已经下载与未下载的网页URL（内存、数据库、缓存数据库） 下载器：通过url下载网页内容；urllib2（Python3中改名urllib）、request（更强大） 解析器：通过一定规则将下载器所得内容解析获取有价值信息；BeautifulSoup(html.parser、lxml )、正则 调度程序爬虫的控制中心，宏观上控制整个爬虫逻辑spider_main.py123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-""" @author: Hodge@func: 爬虫调度程序@time: 2018/3/21 12:00 """from baike_spider import url_manager, html_downloader, html_parser, html_outputerclass SpiderMain(object): def __init__(self): self.urls = url_manager.UrlManager() self.download = html_downloader.HtmlDownloader() self.parser = html_parser.HtmlParser() self.outputer = html_outputer.HtmlOutputer() def craw(self, root_url): count = 1 self.urls.add_new_url(root_url) while self.urls.has_new_url(): try: new_url = self.urls.get_new_url print("craw %d : %s" % (count, new_url)) html_cont = self.download.download(new_url) new_urls, new_data = self.parser.parser(new_url, html_cont) self.urls.add_new_urls(new_urls) self.outputer.collect_data(new_data) count += 1 if count == 20: break except: print("craw failed") self.outputer.output_html()if __name__ == '__main__': root_url = "https://baike.baidu.com/item/Python/407313" obj_spider = SpiderMain() obj_spider.craw(root_url) 下载器通过给定的URL获取对应的网页源代码，并处理编码问题html_downloader.py123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-""" @author: Hodge@file: html_downloader.py @time: 2018/3/21 12:01 """import stringfrom urllib import requestfrom urllib.parse import quoteclass HtmlDownloader(object): def download(self, url): if url is None: return None url_ = quote(url, safe=string.printable) response = request.urlopen(url_) if response.getcode() != 200: return None return response.read().decode("utf-8")if __name__ == '__main__': hd = HtmlDownloader() html = hd.download("https://baike.baidu.com/item/Python/407313") print(html) 解析器根据下载器获取的网页源码，解析出下一次爬取的目标网页urls和当前页面的有价值信息html_parser.py12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding: utf-8 -*-""" @author: Hodge@file: html_parser.py @time: 2018/3/21 12:01 """import reimport urllib.parse as urlparsefrom bs4 import BeautifulSoupclass HtmlParser(object): def parser(self, page_url, html_cont): if page_url is None or html_cont is None: return soup = BeautifulSoup(html_cont, "html.parser") new_urls = self._get_new_urls(page_url, soup) new_data = self._get_new_data(page_url, soup) return new_urls, new_data def _get_new_urls(self, page_url, soup): new_urls = set() links = soup.find_all('a', href=re.compile(r"/item/")) for link in links: new_url = link['href'] new_full_url = urlparse.urljoin(page_url, new_url) new_urls.add(new_full_url) return new_urls def _get_new_data(self, page_url, soup): res_data = &#123;'url': page_url&#125; # &lt;dd class="lemmaWgt-lemmaTitle-title"&gt; &lt;h1&gt;Python&lt;/h1&gt; title_node = soup.find('dd', class_="lemmaWgt-lemmaTitle-title").find('h1') res_data['title'] = title_node.get_text() summary_node = soup.find('div', class_="lemma-summary") res_data['summary'] = summary_node.get_text() return res_data url管理器管理已经下载与未下载的网页URLs，同时负责url的拼接等操作。url_manager.py12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-""" @author: Hodge@file: url_manager.py @time: 2018/3/21 12:01 """class UrlManager(object): def __init__(self): self.new_urls = set() self.old_urls = set() def add_new_url(self, url): if url is None: return if url not in self.new_urls and url not in self.old_urls: self.new_urls.add(url) def has_new_url(self): return len(self.new_urls) != 0 @property def get_new_url(self): new_url = self.new_urls.pop() self.old_urls.add(new_url) return new_url def add_new_urls(self, urls): if urls is None or len(urls) == 0: return for url in urls: self.add_new_url(url) 结果输出将解析的有价值数据格式化输出保存到指定文件，注意编码问题。html_outputer.py123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*-""" @author: Hodge@file: html_outputer.py @time: 2018/3/21 12:02 """class HtmlOutputer(object): def __init__(self): self.datas = [] def collect_data(self, data): if data is None: return self.datas.append(data) def output_html(self): fout = open('output.html', 'w', encoding='utf-8') fout.write("&lt;html&gt;") fout.write("&lt;body&gt;") fout.write("&lt;a&gt;") for data in self.datas: # 屏蔽掉原来的，重新写一个更美观的输出效果 # fout.write("&lt;tr&gt;") # fout.write("&lt;td&gt;%s&lt;/td&gt;" % data['url']) # fout.write("&lt;td&gt;%s&lt;/td&gt;" % data['title']) # fout.write("&lt;td&gt;%s&lt;/td&gt;" % data['summary']) # fout.write("&lt;/tr&gt;") fout.write('&lt;a href="%s"&gt;%s&lt;/a&gt;' % (data['url'], data['title'])) fout.write('&lt;p&gt;%s&lt;/p&gt;' % data['summary']) fout.write("&lt;/a&gt;") fout.write("&lt;/body&gt;") fout.write("&lt;/html&gt;") fout.close() 这只是最简单基础的爬虫结构，将来可在此基础上针对性扩展。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sort]]></title>
    <url>%2F2018%2F03%2F15%2Fsort%2F</url>
    <content type="text"><![CDATA[简介排序：将数组中的元素按指定的大小顺序排列好排序算法类模板： void sort(Comparable[] a) //排序算法 boolean less(Comparable v,Comparable w) //比较两个元素大小 void exch(Comparable[] a,int i,int j) //交换两个元素位置 void show(Comparable[] a) //打印数组 boolean isSorted(Comparable[] a) //判断数组是否已排序完成选择排序简介数组a[],长度为N；从左i=0处往右依次进行：选择i到N里最小的数放在i处代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import edu.princeton.cs.algs4.In;import edu.princeton.cs.algs4.StdOut;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;/** * 选择排序 * 从左往右依次进行 * 选择i到a.length里最小的数放在i处 */public class Selection&#123; public static void sort(Comparable[] a) //排序算法 &#123; int N = a.length; for (int i = 0; i &lt; N; i++) &#123; int min = i; for (int j = i + 1; j &lt; N; j++) &#123; if (less(a[j], a[min])) &#123; min = j; &#125; &#125; exch(a, i, min); &#125; &#125; public static void main(String[] args) throws IOException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); String inputStr = br.readLine(); String[] a = inputStr.split(" "); sort(a); assert isSorted(a); show(a); &#125; private static boolean less(Comparable v, Comparable w) //比较两个元素大小 &#123; return v.compareTo(w) &lt; 0; &#125; private static void exch(Comparable[] a, int i, int j) //交换两个元素位置 &#123; Comparable t = a[i]; a[i] = a[j]; a[j] = t; &#125; private static void show(Comparable[] a) //打印数组 &#123; for (int i = 0; i &lt; a.length; i++) &#123; StdOut.print(a[i] + " "); &#125; StdOut.println(); &#125; public static boolean isSorted(Comparable[] a) //判断数组是否已排序完成 &#123; for (int i = 0; i &lt; a.length; i++) &#123; if (less(a[i], a[i - 1])) &#123; return false; &#125; &#125; return true; &#125;&#125; 分析对于长度为N的数组，从0到N-1的任意i都会进行一次交换和N-1-i次比较所以总共有N次交换及(N+1)+(N+2)+…+2+1=N(N-1)/2~N^2/2次比较。特点：交换操作少，比较次数过多。 插入排序简介从左往右依次进行将i与i到0的数据依次比较，直到将i移动到适当位置。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import edu.princeton.cs.algs4.StdOut;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;/** * 插入排序 * 从左往右依次进行 * 将i与i到0的数据依次比较，直到将i移动到适当位置。 */public class Insertion&#123; public static void sort(Comparable[] a) //排序算法 &#123; int N = a.length; for (int i = 1; i &lt; N; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; less(a[j], a[j - 1]); j++) &#123; exch(a, j, j - 1); &#125; &#125; &#125; public static void main(String[] args) throws IOException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); String inputStr = br.readLine(); String[] a = inputStr.split(" "); sort(a); assert isSorted(a); show(a); &#125; private static boolean less(Comparable v, Comparable w) //比较两个元素大小 &#123; return v.compareTo(w) &lt; 0; &#125; private static void exch(Comparable[] a, int i, int j) //交换两个元素位置 &#123; Comparable t = a[i]; a[i] = a[j]; a[j] = t; &#125; private static void show(Comparable[] a) //打印数组 &#123; for (int i = 0; i &lt; a.length; i++) &#123; StdOut.print(a[i] + " "); &#125; StdOut.println(); &#125; public static boolean isSorted(Comparable[] a) //判断数组是否已排序完成 &#123; for (int i = 0; i &lt; a.length; i++) &#123; if (less(a[i], a[i - 1])) &#123; return false; &#125; &#125; return true; &#125;&#125; 分析对于长度为N且主键不重复的数组，最坏情况下需要~N^2/2次比较和~N^2/2次交换最好情况下需要N-1次比较，0次交换。对部分有序数组十分高效。 实验比较T次，每次用长度为N的随机数组，测试两种排序方法的效率。 代码1234567891011121314151617181920212223242526272829303132333435363738394041import edu.princeton.cs.algs4.StdOut;import edu.princeton.cs.algs4.StdRandom;import edu.princeton.cs.algs4.Stopwatch;public class SortCompare&#123; public static double time(String alg,Double[] a) &#123;//为排序算法计时 Stopwatch timer = new Stopwatch(); if(alg.equals("Insertion")) Insertion.sort(a); if(alg.equals("Selection")) Selection.sort(a); return timer.elapsedTime(); &#125; public static double timeRandomInput(String alg,int N,int T) &#123; double total = 0.0; Double[] a = new Double[N]; for(int t=0;t&lt;T;t++) &#123; for (int i = 0; i &lt; N; i++) &#123; a[i] = StdRandom.uniform(); &#125; total += time(alg,a); &#125; return total; &#125; public static void main(String[] args) &#123; String alg1 = "Insertion"; String alg2 = "Selection"; int N = 100; int T = 1000; double t1 = timeRandomInput(alg1,N,T); double t2 = timeRandomInput(alg2,N,T); StdOut.printf("%f,%f\n",t1,t2); StdOut.printf("%d 次，每次%d长度数组",T,N); StdOut.printf("插入排序比选择排序快%.1f倍",t2/t1); &#125;&#125; 对 1000 组长度为 1000 的数组进行排序测试Selection用时：1.48Insertion用时：1.22Insertion 比 Selection 快 1.2倍 希尔排序一个h有序数组就是h个互相独立的有序数组编织在一起的数组按照增量序列依次将数组以增量h进行普通插入排序，直到h=1完成排序 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import edu.princeton.cs.algs4.StdOut;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;/** * 希尔排序 * 一个h有序数组就是h个互相独立的有序数组编织在一起的数组 * 按照增量序列依次将数组以增量h进行普通插入排序，直到h=1完成排序 */public class Shell&#123; public static void sort(Comparable[] a) //排序算法 &#123; int N = a.length; int h = 1; while (h &lt; N / 3) &#123; h = 3 * h + 1; &#125; while (h &gt;= 1) &#123; for (int i = h; i &lt; N; i++) &#123; for (int j = i; j &gt;= h &amp;&amp; less(a[j], a[j - h]); j -= h) &#123; exch(a, j, j - h); &#125; &#125; h /= 3; &#125; &#125; public static void main(String[] args) throws IOException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); String inputStr = br.readLine(); String[] a = inputStr.split(" "); Integer[] aa = new Integer[a.length]; int i = 0; for (String str:a) &#123; aa[i++] = Integer.parseInt(str); &#125; sort(aa); assert isSorted(aa); show(aa); &#125; private static boolean less(Comparable v, Comparable w) //比较两个元素大小 &#123; return v.compareTo(w) &lt; 0; &#125; private static void exch(Comparable[] a, int i, int j) //交换两个元素位置 &#123; Comparable t = a[i]; a[i] = a[j]; a[j] = t; &#125; private static void show(Comparable[] a) //打印数组 &#123; for (int i = 0; i &lt; a.length; i++) &#123; StdOut.print(a[i] + " "); &#125; StdOut.println(); &#125; public static boolean isSorted(Comparable[] a) //判断数组是否已排序完成 &#123; for (int i = 0; i &lt; a.length; i++) &#123; if (less(a[i], a[i - 1])) &#123; return false; &#125; &#125; return true; &#125;&#125; 分析希尔排序的关键在于增量序列的选取，暂无法准确描述其对于乱序的数组的性能特征。希尔排序对于插入排序，数组越大优势越大： 对 1000 组长度为 1000 的数组进行排序测试Shell用时：0.20Insertion用时：1.36Shell 比 Insertion 快 6.7倍 归并排序简介递归地将数组分成两半分别排序，然后将结果归并（即将两个有序数组合并成一个有序数组）起来。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * Description: 归并排序 * Author: Hodge * Date: 2018-03-25 */public class Merge&#123; //将以mid分隔的两个有序数组合并成一个有序数组 public static void merge(Comparable[] a, int lo, int mid, int hi) &#123; int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) &#123; a[k] = aux[j++]; &#125; else if (j &gt; hi) &#123; a[k] = aux[i++]; &#125; else if (less(aux[j], aux[i])) &#123; a[k] = aux[j++]; &#125; else &#123; a[k] = aux[i++]; &#125; &#125; &#125; private static Comparable[] aux; public static void sort(Comparable[] a) &#123; aux = new Comparable[a.length]; sortTopDown(a, 0, a.length - 1); &#125; //自顶向下的归并排序 public static void sortTopDown(Comparable[] a, int lo, int hi) &#123; if (lo &gt;= hi) &#123; return; &#125; int mid = lo + (hi - lo) / 2; sortTopDown(a, lo, mid); sortTopDown(a, mid + 1, hi); merge(a, lo, mid, hi); &#125; //自底向上的归并排序 public static void sortDownTop(Comparable[] a) &#123; int N = a.length; aux = new Comparable[N]; for (int sz = 1; sz &lt; N; sz = sz + sz) &#123; for (int lo = 0; lo &lt; N - sz; lo += sz + sz) &#123; merge(a, lo, lo + sz - 1, Math.min(lo + sz + sz - 1, N - 1)); &#125; &#125; &#125; private static boolean less(Comparable v, Comparable w) //比较两个元素大小 &#123; return v.compareTo(w) &lt; 0; &#125;&#125; 分析对于长度为N的任意数组，归并排序需要0.5NlgN至NlgN次比较，最多需要访问数组6N*lgN次。 对 1000 组长度为 10000 的数组进行排序测试Shell用时：2.22Merge用时：1.76Merge 比 Shell 快 1.3倍 对 1000 组长度为 10000 的数组进行排序测试Shell用时：2.33MergeDownTop用时：1.76MergeDownTop 比 Shell 快 1.3倍 对 1000 组长度为 10000 的数组进行排序测试Merge用时：1.83MergeDownTop用时：1.63MergeDownTop 比 Merge 快 1.1倍 对 10000 组长度为 10000 的数组进行排序测试Merge用时：17.07MergeDownTop用时：16.47MergeDownTop 比 Merge 快 1.0倍 快速排序经典快排从第一个元素v起，同时从数组两端遍历，将大于和小于v的元素互换位置。然后将v分割出来的两个子数组分别递归做上述操作。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import edu.princeton.cs.algs4.StdRandom;/** * Description: 快速排序 * Author: Hodge * Date: 2018-03-26 */public class Quick&#123; public static void sort(Comparable[] a) &#123; StdRandom.shuffle(a); sort(a, 0, a.length - 1); &#125; private static void sort(Comparable[] a, int lo, int hi) &#123;//经典快速排序 if (lo &gt;= hi) &#123; return; &#125; int j = partition(a, lo, hi); sort(a, lo, j - 1); sort(a, j + 1, hi); &#125; private static int partition(Comparable[] a, int lo, int hi) &#123;//将数组切分为a[lo..i-1],a[i],a[i+1..hi] int i = lo, j = hi + 1; Comparable v = a[lo]; while (true) &#123; while (less(a[++i], v)) &#123; if (i == hi) &#123; break; &#125; &#125; while (less(v, a[--j])) &#123; if (j == lo) &#123; break; &#125; &#125; if (i &gt;= j) &#123; break; &#125; exch(a, i, j); &#125; exch(a, lo, j); return j; &#125; private static boolean less(Comparable v, Comparable w) //比较两个元素大小 &#123; return v.compareTo(w) &lt; 0; &#125; private static void exch(Comparable[] a, int i, int j) //交换两个元素位置 &#123; Comparable t = a[i]; a[i] = a[j]; a[j] = t; &#125;&#125; 分析将长度为N的无重复数组排序，快排平均需要~2NlnN次比较（约等于1.39NlgN）快排最多需要约N^2/2次比较，但随机打乱数组能预防这种情况。 对 10000 组长度为 10000 的数组进行排序测试Merge用时：16.66Quick用时：11.89Quick 比 Merge 快 1.4倍 三向切分的快速排序对于有大量重复数据的大型数组，在快速排序的基础上，将数组分为大于、等于、小于v的三部分递归时仅对大于、小于v的数组进行排序，即可在此特定条件下获得更优的表现。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445import edu.princeton.cs.algs4.StdRandom;/** * Description: 三向切分的快速排序 * Author: Hodge * Date: 2018-03-26 */public class Quick3way&#123; public static void sort(Comparable[] a) &#123; StdRandom.shuffle(a); sort(a, 0, a.length - 1); &#125; private static void sort(Comparable[] a, int lo, int hi) &#123; if (lo &gt;= hi) &#123; return; &#125; int lt = lo, i = lo + 1, gt = hi; Comparable v = a[lo]; while (i &lt;= gt) &#123; int cmp = a[i].compareTo(v); if (cmp &lt; 0) &#123; exch(a, lt++, i++); &#125; else if (cmp &gt; 0) &#123; exch(a, i, gt--); &#125; else i++; &#125; sort(a, lo, lt - 1); sort(a, gt + 1, hi); &#125; private static void exch(Comparable[] a, int i, int j) //交换两个元素位置 &#123; Comparable t = a[i]; a[i] = a[j]; a[j] = t; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并查集union_find]]></title>
    <url>%2F2018%2F03%2F13%2Funion-find%2F</url>
    <content type="text"><![CDATA[动态连通性问题的输入是一列整数对，其中每个整数都表示一个某种类型的对象，一对整数p、q可以被理解为“p和q是相连的”。我们假设“相连”是一种等价关系，这也就意味着它具有： 自反性：p和p是相连的。对称性：如果p和q是相连的，那么q和p也是相连的。传递性：如果p和q是相连的且q和r是相连的，那么p和r也是相连的。 等价关系能够将对象分为多个等价类。在这里，当且仅当两个对象相连时它们才属于同一个等价类。我们的目标是编写一个程序来过滤掉序列中所有无意义的整数对（两个整数均来自于同一个等价类中）。换句话说，当程序从输入中读取了整数对p和q时，如果已知的所有整数对都不能说明p和q是相连的，那么则将这一对整数写入到输出中。如果已知的数据可以说明p和q是相连的，那么程序应该忽略p、q这对整数并继续处理输入中的下一对整数 quick-find简介所有N个数据对应数组id[N]，初始化数组id[i]=i;即第i个数据的所属分量为i;对于输入的数据对p,q:先判断是否在同一分量，否的话调用union方法：即将后者的分量内所有数据的分量标识改为前者的分量标识。 UF.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import edu.princeton.cs.algs4.StdIn;import edu.princeton.cs.algs4.StdOut;public class UF&#123; private int[] id;// 分量id private int count;// 分量总数 public UF(int N) &#123; count = N; id = new int[N]; for (int i = 0; i &lt; N; i++) &#123; id[i] = i; &#125; &#125; public int count()// 返回分量总数 &#123; return count; &#125; public boolean connected(int p, int q)// p,q是否在同一分量 &#123; return find(p) == find(q); &#125; public int find(int p)// p所在分量的标识符 &#123; return id[p]; &#125; public void union(int p, int q)// 连接p,q &#123; int pId = find(p); int qId = find(q); for (int i = 0; i &lt; id.length; i++) &#123; if (id[i] == pId) &#123; id[i] = qId; &#125; &#125; count--; &#125; public static void main(String[] args) &#123; int N = StdIn.readInt(); UF uf = new UF(N); int flag = 0;// 输入计数器 while (!StdIn.isEmpty()) &#123; int p = StdIn.readInt(); int q = StdIn.readInt(); flag++; if (uf.connected(p, q)) &#123; continue; &#125; uf.union(p, q); if (flag % 100 == 0) &#123; System.out.println(flag); &#125; &#125; StdOut.println(N); StdOut.println(uf.count() + "components"); &#125;&#125; 分析假设使用本算法解决动态连通性问题并且最后只得到一个连通分量，那么一共要调用N-1次union()操作。每次connected()会调用两次find(),每次find()调用要访问一次数组，而归并两个分量的union()操作会调用两次find()所以该情况下至少要进行(2+2+N-1)*(N-1)~N^2次数组访问。 显然该算法在largeUF.txt(100万级别的输入)条件下运行时间过长。 quick-union简介每个触点所对应的id[]元素都是同一个分量中的另一个触点的名称（也可能是它自己）所有N个数据对应数组id[N]，初始化数组id[i]=i;即第i个数据的所属分量为i;对于输入的数据对p,q:先判断是否在同一分量，否的话调用union方法：即跟随链接到达各自的根触点，然后将一个根触点链接到另一根触点即实现一个分量重命名为另一个分量。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import edu.princeton.cs.algs4.StdIn;import edu.princeton.cs.algs4.StdOut;public class UF&#123; private int[] id;// 分量id private int count;// 分量总数 public UF(int N) &#123; count = N; id = new int[N]; for (int i = 0; i &lt; N; i++) &#123; id[i] = i; &#125; &#125; public int count()// 返回分量总数 &#123; return count; &#125; public boolean connected(int p, int q)// p,q是否在同一分量 &#123; return find(p) == find(q); &#125; public int find(int p)// p所在分量的标识符 &#123; while(p != id[p]) &#123; p = id[p]; &#125; return p; &#125; public void union(int p, int q)// 连接p,q &#123; int pRoot = find(p); int qRoot = find(q); id[pRoot] = qRoot; count--; &#125; public static void main(String[] args) &#123; int N = StdIn.readInt(); UF uf = new UF(N); int flag = 0;// 输入计数器 while (!StdIn.isEmpty()) &#123; int p = StdIn.readInt(); int q = StdIn.readInt(); flag++; if (uf.connected(p, q)) &#123; continue; &#125; uf.union(p, q); if (flag % 100 == 0) &#123; System.out.println(flag); &#125; &#125; StdOut.println(N); StdOut.println(uf.count() + "components"); &#125;&#125; 分析find()方法访问数组的次数是给定触点在树中深度的两倍再加1，union()和connected()访问数组的次数分别为两次find()+1和两次find()操作。假设使用本算法解决动态连通性问题并且最后只得到一个连通分量，对于最底层数与root数用union()进行的数组访问次数为2N+1+2,因此处理整个N对数据所需的访问数组次数为2(1+2+…+N)~N^2。 加权quick-union简介在quick-union的基础上，更改union方法，使得每次将较小的树连接在较大树的根节点上以此减少整棵树的高度，故而减少数组访问次数 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import edu.princeton.cs.algs4.StdIn;import edu.princeton.cs.algs4.StdOut;public class UF&#123; private int[] id;// 分量id private int[] sz;// 各节点所对应分量的大小 private int count;// 分量总数 public UF(int N) &#123; count = N; id = new int[N]; for (int i = 0; i &lt; N; i++) &#123; id[i] = i; &#125; sz = new int[N]; for (int i = 0; i &lt; N; i++) &#123; sz[i] = 1; &#125; &#125; public int count()// 返回分量总数 &#123; return count; &#125; public boolean connected(int p, int q)// p,q是否在同一分量 &#123; return find(p) == find(q); &#125; public int find(int p)// p所在分量的标识符 &#123; while (p != id[p]) &#123; p = id[p]; &#125; return p; &#125; public void union(int p, int q)// 连接p,q &#123; int i = find(p); int j = find(q); if (sz[i] &lt; sz[j]) &#123; id[i] = j; sz[j] += sz[i]; &#125; else &#123; id[j] = i; sz[i] += sz[j]; &#125; count--; &#125; public static void main(String[] args) &#123; int N = StdIn.readInt(); UF uf = new UF(N); int flag = 0;// 输入计数器 while (!StdIn.isEmpty()) &#123; int p = StdIn.readInt(); int q = StdIn.readInt(); flag++; if (uf.connected(p, q)) &#123; continue; &#125; uf.union(p, q); if (flag % 100 == 0) &#123; System.out.println(flag); &#125; &#125; StdOut.println(N); StdOut.println(uf.count() + "components"); &#125;&#125; 分析加权quick-union算法在处理N个触点和M条连接时最多访问数组cMlgN次，c为常数。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>union-find</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch_base]]></title>
    <url>%2F2018%2F01%2F30%2Fpytorch-base%2F</url>
    <content type="text"><![CDATA[引用自github. Tensor创建一个随机的二维数组（矩阵）123exam1 = torch.randn(2, 3)exam2 = torch.zeros(2, 3)exam3 = torch.ones(2, 3) 当然，我们也可以直接从python的数组直接构造exam4 = torch.Tensor([[1, 2, 4], [2, 3, 6] ])numpy 通常是通过 .shape 来获取数组的形状，但是对于torch.Tensor，使用的是 .size()需要对数组形状进行改变，我们可以采用 .view() 的方式exam5 = exam4.view(3, 2)-1表示的是系统自动补齐exam6 = exam4.view(1, -1) torch.Tensor 支持大量的数学操作符 + , - , * , / 都是可以用的。使用add函数会生成一个新的Tensor变量， add 函数会直接再当前Tensor变量上进行操作所以，对于函数名末尾带有”“ 的函数都是会对Tensor变量本身进行操作的 对于常用的矩阵运算Tensor也有很好的支持123456exam7 = torch.Tensor([[1, 2, 3], [4, 5, 6]])exam8 = torch.randn(2, 3)# 矩阵乘法, 其中 t() 表示取转置torch.mm(exam7, exam8.t())# 矩阵对应元素相乘exam7 * exam8 函数名后面带下划线 的函数会修改Tensor本身。例如，x.add(y)和x.t_()会改变 x，但x.add(y)和x.t()返回一个新的Tensor， 而x不变。 跟numpy一样，Tensor中也存在Broadcasting123456789101112131415161718192021a = torch.arange(0, 3).view(3, 1)b = torch.arange(0, 2).view(1, 2)print("a:", a)print("b:", b)print("a+b:", a + b)a: 0 1 2[torch.FloatTensor of size 3x1]b: 0 1[torch.FloatTensor of size 1x2]a+b: 0 1 1 2 2 3[torch.FloatTensor of size 3x2] Tensor和Numpy的相互转换1234567891011import torchimport numpy as npx = np.ones((2, 3))print(x)y = torch.from_numpy(x) # 从numpy -&gt; torch.Tensorprint(y)z = y.numpy() # 从torch.Tensor -&gt; numpyprint(z) Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。 Tensor可通过.cuda 方法转为GPU的Tensor，从而享受GPU带来的加速运算。1234if t.cuda.is_available(): x = x.cuda() y = y.cuda() x + y torch.unsqueezetorch.unsqueeze(input, dim, out=None)返回一个新的张量，对输入的指定位置插入维度 1注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。如果dim为负，则将会被转化dim+input.dim()+1参数:tensor (Tensor) – 输入张量dim (int) – 插入维度的索引out (Tensor, optional) – 结果张量12345678910&gt;&gt;&gt; x = torch.Tensor([1, 2, 3, 4])&gt;&gt;&gt; torch.unsqueeze(x, 0) 1 2 3 4[torch.FloatTensor of size 1x4]&gt;&gt;&gt; torch.unsqueeze(x, 1) 1 2 3 4[torch.FloatTensor of size 4x1] autograd在机器学习中，我们通常使用梯度下降（gradient descent）来更新模型参数从而求解。损失函数关于模型参数的梯度指向一个可以降低损失函数值的方向，我们不断地沿着梯度的方向更新模型从而最小化损失函数。虽然梯度计算比较直观，但对于复杂的模型，例如多达数十层的神经网络，手动计算梯度非常困难。 PyTorch中提供给了我们自动化求导的包 —— autograd 导入之后，我们用一个 Variable X 对象来包装Tensor变量。然后 X.data 就是该Tensor变量，X.grad 就是梯度12345678910111213import torchfrom torch.autograd import Variabledef fn(x): y = 3 * x.pow(3) + 4 * x.pow(2) + 6 return yx1 = Variable(torch.Tensor([1]), requires_grad=True)y1 = fn(x1)print(y1)y1.backward() # 自动求导print(x1.grad) # 查看梯度 需要注意的一点是：如果我们输入的 Tensor 不是一个标量，而是矢量（多个值）。那么，我们在调用backward()之前，需要让结果变成标量,才能求出导数。1234567891011import torchfrom torch.autograd import Variabledef fn(x): y = 3 * x.pow(3) + 4 * x.pow(2) + 6 return yx2 = Variable(torch.Tensor([[1, 2], [3, 4]]), requires_grad=True)y2 = fn(x2).mean() # 将结果变成标量，这样就不会报错了y2.backward() # 自动求导print(x2.grad) Variable和Tensor具有近乎一致的接口，在实际使用中可以无缝切换。123x = Variable(t.ones(4,5))y = t.cos(x)x_tensor_cos = t.cos(x.data) 定义网络把网络中具有可学习参数的层放在构造函数init中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放，但建议不放在其中，而在forward中使用nn.functional代替。网络的可学习参数通过net.parameters()返回，net.named_parameters可同时返回可学习的参数及名称。forward函数的输入和输出都是Variable，只有Variable才具有自动求导功能，而Tensor是没有的，所以在输入时，需把Tensor封装成Variable。 简单例子下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下: 使用torchvision加载并预处理CIFAR-10数据集 定义网络 定义损失函数和优化器 训练网络并更新网络参数 测试网络 CIFAR-10数据加载及预处理CIFAR-10^3是一个常用的彩色图片数据集，它有10个类别: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。每张图片都是 3×32×323×32×32 ，也即3-通道彩色图片，分辨率为 32×3232×32 。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import torchvision as tvimport torchvision.transforms as transformsfrom torchvision.transforms import ToPILImageshow = ToPILImage() # 可以把Tensor转成Image，方便可视化# 第一次运行程序torchvision会自动下载CIFAR-10数据集，# 大约100M，需花费一定的时间，# 如果已经下载有CIFAR-10，可通过root参数指定# 定义对数据的预处理transform = transforms.Compose([ transforms.ToTensor(), # 转为Tensor transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化 ])# 训练集trainset = tv.datasets.CIFAR10( root='/home/cy/tmp/data/', train=True, download=True, transform=transform)trainloader = t.utils.data.DataLoader( trainset, batch_size=4, shuffle=True, num_workers=2)# 测试集testset = tv.datasets.CIFAR10( '/home/cy/tmp/data/', train=False, download=True, transform=transform)testloader = t.utils.data.DataLoader( testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')# Dataset对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。(data, label) = trainset[100]print(classes[label])# (data + 1) / 2是为了还原被归一化的数据show((data + 1) / 2).resize((100, 100))# Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代。dataiter = iter(trainloader)images, labels = dataiter.next() # 返回4张图片及标签print(' '.join('%11s'%classes[labels[j]] for j in range(4)))show(tv.utils.make_grid((images+1)/2)).resize((400,100)) 常用神经网络层图像相关层图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维(1D)、二维(2D)、三维（3D），池化方式又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。1234567from PIL import Imagefrom torchvision.transforms import ToTensor, ToPILImageto_tensor = ToTensor() # img -&gt; tensorto_pil = ToPILImage()lena = Image.open('imgs/lena.png')# 输入是一个batch，batch_size＝1input = to_tensor(lena).unsqueeze(0) 池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。除了卷积层和池化层，深度学习中还将常用到以下几个层： Linear：全连接层。 BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。 Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。 下面通过例子来说明它们的使用。]]></content>
      <categories>
        <category>PyTorch笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM]]></title>
    <url>%2F2018%2F01%2F30%2FLSTM%2F</url>
    <content type="text"><![CDATA[需求利用循环神经网络的LSTM网络实现对mnist手写数字图片的识别其中用到了批训练方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import torchfrom torch import nnfrom torch.autograd import Variableimport torchvision.datasets as dsetsimport torchvision.transforms as transforms# torch.manual_seed(1) # reproducible# Hyper ParametersEPOCH = 1 # train the training data n times, to save time, we just train 1 epochBATCH_SIZE = 64TIME_STEP = 28 # rnn time step / image heightINPUT_SIZE = 28 # rnn input size / image widthLR = 0.01 # learning rateDOWNLOAD_MNIST = False # set to True if haven't download the data# Mnist digital datasettrain_data = dsets.MNIST( root='./mnist/', train=True, # this is training data transform=transforms.ToTensor(), # Converts a PIL.Image or numpy.ndarray to # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0] download=DOWNLOAD_MNIST, # download it if you don't have it)# Data Loader for easy mini-batch return in trainingtrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)# convert test data into Variable, pick 2000 samples to speed up testingtest_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())test_x = Variable(test_data.test_data, volatile=True).type(torch.FloatTensor)[:2000]/255. # shape (2000, 28, 28) value in range(0,1)test_y = test_data.test_labels.numpy().squeeze()[:2000] # covert to numpy arrayclass RNN(nn.Module): def __init__(self): super(RNN, self).__init__() self.rnn = nn.LSTM( # if use nn.RNN(), it hardly learns input_size=INPUT_SIZE, hidden_size=64, # rnn hidden unit num_layers=1, # number of rnn layer batch_first=True, # input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size) ) self.out = nn.Linear(64, 10) def forward(self, x): # x shape (batch, time_step, input_size) # r_out shape (batch, time_step, output_size) # h_n shape (n_layers, batch, hidden_size) # h_c shape (n_layers, batch, hidden_size) r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state # choose r_out at the last time step out = self.out(r_out[:, -1, :]) return outrnn = RNN()print(rnn)optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parametersloss_func = nn.CrossEntropyLoss() # the target label is not one-hotted# training and testingfor epoch in range(EPOCH): for step, (x, y) in enumerate(train_loader): # gives batch data b_x = Variable(x.view(-1, 28, 28)) # reshape x to (batch, time_step, input_size) b_y = Variable(y) # batch y output = rnn(b_x) # rnn output loss = loss_func(output, b_y) # cross entropy loss optimizer.zero_grad() # clear gradients for this training step loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if step % 50 == 0: test_output = rnn(test_x) # (samples, time_step, input_size) pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze() accuracy = sum(pred_y == test_y) / float(test_y.size) print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)# print 10 predictions from test datatest_output = rnn(test_x[:10].view(-1, 28, 28))pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()print(pred_y, 'prediction number')print(test_y[:10], 'real number')]]></content>
      <categories>
        <category>PyTorch笔记</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN]]></title>
    <url>%2F2018%2F01%2F30%2FCNN%2F</url>
    <content type="text"><![CDATA[需求利用卷积神经网络实现对mnist手写数字图片的识别其中用到了批训练方法 CNN.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import os# third-party libraryimport torchimport torch.nn as nnfrom torch.autograd import Variableimport torch.utils.data as Dataimport torchvision# torch.manual_seed(1) # reproducible# Hyper ParametersEPOCH = 1 # train the training data n times, to save time, we just train 1 epochBATCH_SIZE = 50LR = 0.001 # learning rateDOWNLOAD_MNIST = False# Mnist digits datasetif not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'): # not mnist dir or mnist is empyt dir DOWNLOAD_MNIST = Truetrain_data = torchvision.datasets.MNIST( root='./mnist/', train=True, # this is training data transform=torchvision.transforms.ToTensor(), # Converts a PIL.Image or numpy.ndarray to # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0] download=DOWNLOAD_MNIST,)# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)# convert test data into Variable, pick 2000 samples to speed up testingtest_data = torchvision.datasets.MNIST(root='./mnist/', train=False)test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255. # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)test_y = test_data.test_labels[:2000]class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.conv1 = nn.Sequential( # input shape (1, 28, 28) nn.Conv2d( in_channels=1, # input height out_channels=16, # n_filters kernel_size=5, # filter size stride=1, # filter movement/step padding=2, # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1 ), # output shape (16, 28, 28) nn.ReLU(), # activation nn.MaxPool2d(kernel_size=2), # choose max value in 2x2 area, output shape (16, 14, 14) ) self.conv2 = nn.Sequential( # input shape (1, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), # output shape (32, 14, 14) nn.ReLU(), # activation nn.MaxPool2d(2), # output shape (32, 7, 7) ) self.out = nn.Linear(32 * 7 * 7, 10) # fully connected layer, output 10 classes def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) # flatten the output of conv2 to (batch_size, 32 * 7 * 7) output = self.out(x) return output, x # return x for visualizationcnn = CNN()optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) # optimize all cnn parametersloss_func = nn.CrossEntropyLoss() # the target label is not one-hotted# training and testingfor epoch in range(EPOCH): for step, (x, y) in enumerate(train_loader): # gives batch data, normalize x when iterate train_loader b_x = Variable(x) # batch x b_y = Variable(y) # batch y output = cnn(b_x)[0] # cnn output loss = loss_func(output, b_y) # cross entropy loss optimizer.zero_grad() # clear gradients for this training step loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if step % 50 == 0: test_output, last_layer = cnn(test_x) pred_y = torch.max(test_output, 1)[1].data.squeeze() accuracy = sum(pred_y == test_y) / float(test_y.size(0)) print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)# print 10 predictions from test datatest_output, _ = cnn(test_x[:10])pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()print(pred_y, 'prediction number')print(test_y[:10].numpy(), 'real number')]]></content>
      <categories>
        <category>PyTorch笔记</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashtag Recommendation for Multimodal Microblog Using Co-Attention Network]]></title>
    <url>%2F2018%2F01%2F26%2Fhashtag-microblog%2F</url>
    <content type="text"><![CDATA[Abstract以往：根据微博文本信息推荐Hashtag本文：多模态信息同时考虑+Attention机制 Introduction以往：只使用文字有时无法准确定位微博的中心主题；部分研究简单地将文字和图片特征加以整合，而事实上正确的hashtag只与文本和图象的部分内容相关联。现在：加入Attention机制，文本、图像相互影响并指导对方特征的提取。一个兼顾文本和图像相互影响的共同关注网络。之前的研究没有考虑图像对文本特征提取的指导意义。注意机制允许模型专注于视觉或文本输入的特定部分，并已成功用于各种多模式模型。在这项工作中，我们采用了从输入推文和图像中选择重要信息的机制。 Feature ExtractionImage feature extraction首先将图片转换成224X224，再用16-layer VGGNet抽取特征，其中将图片分成NxN块区域每块区域获得512维的特征向量 Text feature extraction]]></content>
      <categories>
        <category>读论文</category>
      </categories>
      <tags>
        <tag>读论文</tag>
        <tag>Attention Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[helloos]]></title>
    <url>%2F2018%2F01%2F23%2Fos1%2F</url>
    <content type="text"><![CDATA[本来一无所有，何谈从头再来？读《30天自制操作系统》笔记。第一天。 概述写出操作系统二进制文件，转换成img，再通过模拟器运行。手工写二进制文件太恐怖，故用汇编语言写代码，编译成操作系统二进制文件。故操作系统的hello world版汇编源文件为： ipl.nas123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960; hello-os; TAB=4 ORG 0x7c00 ; 指明程序装载地址; 标准FAT12格式软盘专用的代码 Stand FAT12 format floppy code JMP entry DB 0xeb, 0x4e, 0x90 DB "HELLOIPL" ; 启动扇区名称（8字节） DW 512 ; 每个扇区（sector）大小（必须512字节） DB 1 ; 簇（cluster）大小（必须为1个扇区） DW 1 ; FAT起始位置（一般为第一个扇区） DB 2 ; FAT个数（必须为2） DW 224 ; 根目录大小（一般为224项） DW 2880 ; 该磁盘大小（必须为2880扇区1440*1024/512） DB 0xf0 ; 磁盘类型（必须为0xf0） DW 9 ; FAT的长度（必??9扇区） DW 18 ; 一个磁道（track）有几个扇区（必须为18） DW 2 ; 磁头数（必??2） DD 0 ; 不使用分区，必须是0 DD 2880 ; 重写一次磁盘大小 DB 0,0,0x29 ; 意义不明（固定） DD 0xffffffff ; （可能是）卷标号码 DB "HELLO-OS " ; 磁盘的名称（必须为11字?，不足填空格） DB "FAT12 " ; 磁盘格式名称（必??8字?，不足填空格） RESB 18 ; 先空出18字节; 程序主体entry: MOV AX,0 ; 初始化寄存器 MOV SS,AX MOV SP,0x7c00 MOV DS,AX MOV ES,AX MOV SI,msgputloop: MOV AL,[SI] ADD SI,1 ; 给SI加1 CMP AL,0 JE fin MOV AH,0x0e ; 显示一个文字 MOV BX,15 ; 指定字符颜色 INT 0x10 ; 调用显卡BIOS JMP putloopfin: HLT ; 让CPU停止，等待指令 JMP fin ; 无限循环msg: DB 0x0a, 0x0a ; 换行两次 DB "hello, world" DB 0x0a ; 换行 DB 0 RESB 0x7dfe-$ ; 填写0x00直到0x001fe DB 0x55, 0xaa 在使用光盘里提供的Makefile文件对应的 make run 命令快捷实现.nas文件编译成.bin然后转换成.img最后装载到模拟器运行。在这一过程中Makefile文件里的copy 命令和 del命令皆报“系统找不到指定文件”错误，对应改成cp 与 rm 命令解决。 Makefile1234567891011121314151617181920212223242526272829303132333435363738# 默认动作default : ../z_tools/make.exe img# 镜像文件生成ipl.bin : ipl.nas Makefile ../z_tools/nask.exe ipl.nas ipl.bin ipl.lsthelloos.img : ipl.bin Makefile ../z_tools/edimg.exe imgin:../z_tools/fdimg0at.tek \ wbinimg src:ipl.bin len:512 from:0 to:0 imgout:helloos.img# 其他指令asm : ../z_tools/make.exe -r ipl.binimg : ../z_tools/make.exe -r helloos.imgrun : ../z_tools/make.exe img cp helloos.img ..\z_tools\qemu\fdimage0.bin ../z_tools/make.exe -C ../z_tools/qemuinstall : ../z_tools/make.exe img ../z_tools/imgtol.com w a: helloos.imgclean : -rm ipl.bin -rm ipl.lstsrc_only : ../z_tools/make.exe clean -rm helloos.img 详细代码已上传至github]]></content>
      <categories>
        <category>30*N天自制操作系统</category>
      </categories>
      <tags>
        <tag>汇编</tag>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批训练&优化器]]></title>
    <url>%2F2018%2F01%2F16%2Fpitrain%2F</url>
    <content type="text"><![CDATA[数据先Tensor化再load 批训练.py123456789101112131415161718192021222324import torchimport torch.utils.data as Datatorch.manual_seed(1) # reproducibleBATCH_SIZE = 5# BATCH_SIZE = 8x = torch.linspace(1, 10, 10) # this is x data (torch tensor)y = torch.linspace(10, 1, 10) # this is y data (torch tensor)torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)loader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size=BATCH_SIZE, # mini batch size shuffle=True, # random shuffle for training num_workers=2, # subprocesses for loading data)if __name__ == '__main__': for epoch in range(3): # train entire dataset 3 times for step, (batch_x, batch_y) in enumerate(loader): # for each training step # train your data... print('Epoch: ', epoch, '| Step: ', step, '| batch x: ', batch_x.numpy(), '| batch y: ', batch_y.numpy()) 优化器比较在数据分批训练的基础上比较几种不同优化器的收敛效果 优化器.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import torchimport torch.utils.data as Dataimport torch.nn.functional as Ffrom torch.autograd import Variableimport matplotlib.pyplot as plt# torch.manual_seed(1) # reproducibleLR = 0.01BATCH_SIZE = 32EPOCH = 12# fake datasetx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)y = x.pow(2) + 0.1 * torch.normal(torch.zeros(*x.size()))# plot dataset# plt.scatter(x.numpy(), y.numpy())# plt.show()# put dateset into torch datasettorch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, )# default networkclass Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.hidden = torch.nn.Linear(1, 20) # hidden layer self.predict = torch.nn.Linear(20, 1) # output layer def forward(self, x): x = F.relu(self.hidden(x)) # activation function for hidden layer x = self.predict(x) # linear output return x# different netsnet_SGD = Net()net_Momentum = Net()net_RMSprop = Net()net_Adam = Net()nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]# different optimizersopt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]loss_func = torch.nn.MSELoss()losses_his = [[], [], [], []] # record loss# trainingif __name__ == '__main__': for epoch in range(EPOCH): print('Epoch: ', epoch) for step, (batch_x, batch_y) in enumerate(loader): # for each training step b_x = Variable(batch_x) b_y = Variable(batch_y) for net, opt, l_his in zip(nets, optimizers, losses_his): output = net(b_x) # get output for every net loss = loss_func(output, b_y) # compute loss for every net opt.zero_grad() # clear gradients for next train loss.backward() # backpropagation, compute gradients opt.step() # apply gradients l_his.append(loss.data[0]) # loss recoder labels = ['SGD', 'Momentum', 'RMSprop', 'Adam'] for i, l_his in enumerate(losses_his): plt.plot(l_his, label=labels[i]) plt.legend(loc='best') plt.xlabel('Steps') plt.ylabel('Loss') plt.ylim((0, 0.2)) plt.show()]]></content>
      <categories>
        <category>PyTorch笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LDA主题模型]]></title>
    <url>%2F2018%2F01%2F16%2Flda%2F</url>
    <content type="text"><![CDATA[需求爬到的推文做主题分析方法引用自博客：http://blog.csdn.net/github_36299736/article/details/54966460 简单例子： LDA_demo.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# LDA 例子测试from nltk.tokenize import RegexpTokenizerfrom stop_words import get_stop_wordsfrom nltk.stem.porter import PorterStemmerfrom gensim import corpora, modelsimport gensimtokenizer = RegexpTokenizer(r'\w+')# create English stop words listen_stop = get_stop_words('en')# Create p_stemmer of class PorterStemmerp_stemmer = PorterStemmer()# create sample documentsdoc_a = "Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother."doc_b = "My mother spends a lot of time driving my brother around to baseball practice."doc_c = "Some health experts suggest that driving may cause increased tension and blood pressure."doc_d = "I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better."doc_e = "Health professionals say that brocolli is good for your health."# compile sample documents into a listdoc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]# list for tokenized documents in looptexts = []# loop through document listfor i in doc_set: # clean and tokenize document string raw = i.lower() tokens = tokenizer.tokenize(raw) # remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop] # stem tokens stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens] # add tokens to list texts.append(stemmed_tokens)# turn our tokenized documents into a id &lt;-&gt; term dictionarydictionary = corpora.Dictionary(texts)# convert tokenized documents into a document-term matrixcorpus = [dictionary.doc2bow(text) for text in texts]# generate LDA modelldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)print(ldamodel.print_topics(num_topics=2, num_words=4)) 实现分析最后的实现代码来自：https://github.com/a55509432/python-LDA]]></content>
      <categories>
        <category>工具积累</category>
      </categories>
      <tags>
        <tag>lda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[twitter爬虫2]]></title>
    <url>%2F2018%2F01%2F14%2Fget-twitters%2F</url>
    <content type="text"><![CDATA[需求利用tweepy和hashtag爬取推文（每个hash拿500条左右）对推文进行文本处理：去非英文字符、去链接、去hash、去中止词以hash:twitter形式存字典再存文件：dict_hash_twitter 推文采集由于国内网络环境问题，还是把每个hash对应的推文保存成文件再处理，稳点起见。事实证明，用tweepy包的search方法会有访问频率限制，但我申请了四个twitter app轮回换key还是能实现不间断下载的 get_twitter.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import tweepyfrom tweepy import OAuthHandlerimport relist_consumer_key = ['########################', '########################', '########################', '########################']list_consumer_secret = ['########################', '########################', '########################', '########################']list_access_token = ['########################-########################', '########################-########################', '########################-########################', '########################-########################']list_access_secret = ['########################', '########################', '########################', '########################']num_key = 0consumer_key = list_consumer_key[num_key]consumer_secret = list_consumer_secret[num_key]access_token = list_access_token[num_key]access_secret = list_access_secret[num_key]auth = OAuthHandler(consumer_key, consumer_secret)auth.set_access_token(access_token, access_secret)# api = tweepy.API(auth) # 不使用代理api = tweepy.API(auth, proxy="127.0.0.1:1080")f_hash = open("count_hash.txt", encoding='utf8', errors='ignore')list_hash = []while 1: line_hash = f_hash.readline() if not line_hash: break else: list_hash.append(str(re.findall(r"'(.*?)'", line_hash, flags=0)[0]))print('list_hash生成成功：' + str(len(list_hash)))for id in range(1056, len(list_hash)): try: query = str(list_hash[id]) print(query) # 获取推文 tweets = tweepy.Cursor(api.search, q=query, count=100, lang='en', include_entities=False).items(300) f_twitter = open("./hash_twitter/" + str(list_hash[id]), 'w', encoding='utf8', errors='ignore') i = 0 for tweet in tweets: f_twitter.write(str(tweet.text).replace('\n', ' ') + '\n') i += 1 print(i) if (i &lt; 300): f_short_hash = open("short_hash.txt", 'a', encoding='utf8', errors='ignore') f_short_hash.write(str(list_hash[id]) + '\n') f_short_hash.close() print(id) except(TypeError, tweepy.error.TweepError): print(str(list_hash[id]) + '爬取失败') f_error_hash = open("error_hash.txt", 'a', encoding='utf8', errors='ignore') f_error_hash.write(str(list_hash[id]) + '\n') f_error_hash.close() # 更换key num_key += 1 if (num_key == 4): num_key = 0 consumer_key = list_consumer_key[num_key] consumer_secret = list_consumer_secret[num_key] access_token = list_access_token[num_key] access_secret = list_access_secret[num_key] auth = OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_secret) api = tweepy.API(auth, proxy="127.0.0.1:1080") print('key更换成功，继续下载-------------------------&gt;') 数据处理下载好的文件是多行的推文需要对推文进行文本处理：去非英文字符、去链接、去hash、去中止词然后以hash为文件名以一行处理后的推文为内容存文件 clean_data.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import reimport osdef get_hashs(template): # HashTag以‘#’开头，以空格或回车结尾 copy = False finished = False slotList = [] str = "" for s in template: if s == '#': copy = True elif s == ' ': copy = False finished = True elif s == '\n': copy = False finished = True elif copy: str = str + s if finished: if str != "": slotList.append(str) str = "" finished = False return slotListdef file_name(file_dir): # 获取视频名list for root, dirs, files in os.walk(file_dir): continue return filesdef is_alpha(word): try: return word.encode('ascii').isalpha() except UnicodeEncodeError: return Falsef_stop_word = open("stop_word.txt", encoding='utf8', errors='ignore')list_stop_word = []while 1: line_stop_word = f_stop_word.readline() if not line_stop_word: break else: list_stop_word.append(str(line_stop_word).replace("\n", '').lower())print("停用词列表生成成功:" + str(len(list_stop_word)))list_hash_down = file_name(r'I:\推荐系统\数据集\twitter\hash_twitter')print(len(list_hash_down))for hash in list_hash_down: f_twitter = open('./hash_twitter/' + hash, encoding='utf8', errors='ignore') long_str = '' # for i in range(1, 10): while 1: line_twitter = f_twitter.readline() if not line_twitter: break else: line_twitter = re.sub(r'#\w* ', '', line_twitter) line_twitter = re.sub(r'#\w*\n', '', line_twitter) line_twitter = re.sub(r'https\S* ', '', line_twitter) line_twitter = re.sub(r'https\S*\n', '', line_twitter) text_final = '' for s in line_twitter: if is_alpha(s): text_final += s elif s == ' ' or s == '-' or s == '~': text_final += ' ' # 去除非英文字符，保留空格 text_final = re.sub(r'\s+', ' ', text_final) line_twitter = text_final.lower() list_word = line_twitter.split(' ') # 分词，包含空字符 text_final = '' for word in list_word: if word == '': continue elif word not in list_stop_word: # 去除中止词 text_final += word + ' ' long_str += text_final f_twitter.close() long_str = re.sub(r'\n', '', long_str) long_str = re.sub(r'\s+', ' ', long_str) list_word = long_str.split(' ') if (len(list_word) &lt; 10): print(str(hash) + '太短') f_short_result = open('short_result', 'a', encoding='utf8', errors='ignore') f_short_result.write(str(hash) + '\n') f_short_result.close() else: f_twitter_new = open('./hash_twitter_new/' + hash, 'w', encoding='utf8', errors='ignore') f_twitter_new.write(long_str) f_twitter_new.close()]]></content>
      <categories>
        <category>工具积累</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>twitter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据特征提取]]></title>
    <url>%2F2018%2F01%2F14%2Ffeature-extract%2F</url>
    <content type="text"><![CDATA[在上文处理好的数据集上利用AlexNet和word2vec提取数据特征 test_hdf5_librosa.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216import h5pyimport numpy as npfrom torchvision import models, transformsimport librosaimport osfrom torch.utils.data import Dataset, DataLoaderfrom PIL import Imageimport torch.nn as nnimport torchfrom torch.autograd import Variableimport gensim# -----------------prepare for extract feature--------------------------# -----------------get Alexnet model-------------------------def getAlexNet(DOWNLOAD=True): alexnet = models.alexnet(pretrained=DOWNLOAD) return alexnet# -----------------revise the AlexNet class--------------------------class AlexNet(nn.Module): def __init__(self): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), ) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 6 * 6, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096) ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), 256 * 6 * 6) x = self.classifier(x) return x# -----------------create the dataset class--------------------------class ImageDataset(Dataset): def __init__(self, img_path, transform=None): self.transform = transform self.images = list(map(lambda x: os.path.join(img_path, x), os.listdir(img_path))) def __getitem__(self, index): image_file = self.images[index] image = Image.open(image_file).convert('RGB') if self.transform is not None: image = self.transform(image) return image def __len__(self): return len(self.images)# -----------------create the dataloader--------------------------def get_dataset(path, img_scale, batch_size): transform = transforms.Compose([ transforms.Scale(img_scale), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) dataset = ImageDataset(path, transform) data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=True) return data_loader# -----------------begin to extract feature--------------------------def text_file(path, h5base, id, model): finnal_matrix = None with open(path + 'text.txt', 'r') as f: texts = f.read() text_list = texts.strip().split(" ") for word in text_list: try: vec_word = model[word] except: vec_word = np.zeros(300) vec_word = vec_word.reshape(300, 1) if finnal_matrix is None: finnal_matrix = vec_word else: finnal_matrix = np.concatenate((finnal_matrix, vec_word), axis=1) h5base.create_dataset(id, data=finnal_matrix) return# -----------------get image feature and store--------------------------def image_file(path, h5base, id, model): """ :param path: file path :param h5base: store 12 * 4096 data :return: """ try: img_scale = 224 batch_size = 12 data_loader = get_dataset(path, img_scale, batch_size) for _, data in enumerate(data_loader): # use model to extract feature features = model(Variable(data)) # save features h5base.create_dataset(id, data=features.data.numpy()) except: print(id + "在图像上出现了有问题！") return# -----------------get audio feature and store--------------------------def audio_file(path, h5base, id): """ :param path: file path :param h5base: store 6 * 512 data :return: """ try: finnal_matrix = None use_audio = ["1.mp3", "2.mp3", "3.mp3", "4.mp3", "5.mp3", "6.mp3"] files = librosa.util.find_files(path, recurse=False, ext='mp3') for file in files: file_list = file.split("/") if file_list[-1] not in use_audio: continue y, sr = librosa.load(file) D = librosa.stft(y, n_fft=1022) vec_D = np.mean(D, axis=1, keepdims=True) if finnal_matrix is None: finnal_matrix = vec_D else: finnal_matrix = np.concatenate((finnal_matrix, vec_D), axis=1) # judge final matrix shape if finnal_matrix is None or finnal_matrix.shape != (512, 6): print(id + "有问题！") return h5base.create_dataset(id, data=finnal_matrix) except: print(id + "在音频上出现了有问题！") return# -----------------get audio feature and store--------------------------if __name__ == '__main__': # some config variable root_path = "./dataset1/" DOWNLOAD = False pre_train_weight_alexnet = "./alexnet-owt-4df8aa71.pth" # create h5py file restore_audio_file = h5py.File("dataset1_audio_feature.hdf5", "w") restore_images_file = h5py.File("dataset1_images_feature.hdf5", "w") restore_texts_file = h5py.File("dataset1_texts_feature.hdf5", "w") # -----------------get Alexnet to extrat imgs features-------------------------- # if you alread download the weight, we can make DOWNLOAD = False pre_alexnet = getAlexNet(DOWNLOAD) pre_alexnet.load_state_dict(torch.load(pre_train_weight_alexnet)) pretrain_dict = pre_alexnet.state_dict() alexnet = AlexNet() alexnet_dict = alexnet.state_dict() pretrained_dict = &#123;k: v for k, v in pretrain_dict.items() if k in alexnet_dict&#125; # update the weight of new alexnet alexnet_dict.update(pretrained_dict) # load the new weight alexnet.load_state_dict(alexnet_dict) # -----------------get word2vec by Google to extrat texts features-------------------------- word2vec = gensim.models.KeyedVectors.load_word2vec_format("./GoogleNews-vectors-negative300.bin", binary=True) # go to the dir and loop dirs = os.listdir(root_path) process = 0 total = len(dirs) for dir in dirs: audio_file(root_path + dir + "/audios/", restore_audio_file, dir) image_file(root_path + dir + "/images/", restore_images_file, dir, alexnet) text_file(root_path + dir + "/texts/", restore_texts_file, dir, word2vec) process += 1 if process % (total // 10) == 0: print("alread down") print("Done!")]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[保存提取]]></title>
    <url>%2F2018%2F01%2F09%2Ftorch-save%2F</url>
    <content type="text"><![CDATA[保存提取训练好了一个模型, 我们当然想要保存它, 留到下次要用的时候直接提取直接用 保存提取.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import torchfrom torch.autograd import Variableimport matplotlib.pyplot as plt# fake datax = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)y = x.pow(2) + 0.2 * torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)x, y = Variable(x, requires_grad=False), Variable(y, requires_grad=False)def save(): # save net1 net1 = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1) ) optimizer = torch.optim.SGD(net1.parameters(), lr=0.5) loss_func = torch.nn.MSELoss() for t in range(100): prediction = net1(x) loss = loss_func(prediction, y) optimizer.zero_grad() loss.backward() optimizer.step() # plot result plt.figure(1, figsize=(10, 3)) plt.subplot(131) plt.title('Net1') plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) # 2 ways to save the net torch.save(net1, 'net.pkl') # save entire net torch.save(net1.state_dict(), 'net_params.pkl') # save only the parametersdef restore_net(): # restore entire net1 to net2 net2 = torch.load('net.pkl') prediction = net2(x) # plot result plt.subplot(132) plt.title('Net2') plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)def restore_params(): # restore only the parameters in net1 to net3 net3 = torch.nn.Sequential( torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, 1) ) # copy net1's parameters into net3 net3.load_state_dict(torch.load('net_params.pkl')) prediction = net3(x) # plot result plt.subplot(133) plt.title('Net3') plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5) plt.show()# save net1save()# restore entire net (may slow)restore_net()# restore only the net parametersrestore_params()]]></content>
      <categories>
        <category>PyTorch笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vine数据集处理4]]></title>
    <url>%2F2018%2F01%2F09%2Fclean-down-data%2F</url>
    <content type="text"><![CDATA[需求&amp;前提多个文件夹的视频文件共121117-1862（下载失败数）= 119255 条多个文件夹的分割后文件共119255 - 135（分割失败数）= 119120 条现要对这119120条数据做hash词频统计，并截取词频不小于5的所有数据并清除不合法数据 求得分割后数据数:遍历video文件夹建立list_id并去除extract_fail数据 get_all_videos.py123456789101112131415161718192021222324252627282930313233343536373839404142434445# 获取所有拆分成功的短视频id存入list_id.txtimport osdef file_name(file_dir): # 获取视频名list for root, dirs, files in os.walk(file_dir): continue return filespath = '/home/caoda/Hodge_work_space/videos_data/videos_201712'list_id = []f_list_id = open("list_id.txt", encoding='utf8', errors='ignore')a = f_list_id.read()list_id = eval(a)f_list_id.close()print('读取list_id成功，现有' + str(len(list_id)) + '条数据')list_path = ['07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21', '24']for i in list_path: list_id += file_name(path + i) # print(path + str(i))print('总视频数：' + str(len(list_id))) # 121117条数据-下载失败条数# 去除提取失败与剩余条数f_extract_fail = open("extract_fail", encoding='utf8', errors='ignore')i = 0while 1: line_extract_fail = f_extract_fail.readline() if not line_extract_fail: break else: line_extract_fail = line_extract_fail.replace('\n', '') if (line_extract_fail in list_id): list_id.remove(line_extract_fail) i += 1print('清理' + str(i) + '条数据\n最后总数据' + str(len(list_id)))f_list_id = open("list_id.txt", 'w', encoding='utf8', errors='ignore')f_list_id.write(str(list_id))f_list_id.close()list_id.clear() 词频统计根据list_id和121117长度的final_hashs.txt对最终数据做词频统计写入count_hash.txt count.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# 统计hash总数与词频import collectionsimport redef get_id(template): rule = r'(.*?)::::' slot_list = re.findall(rule, template) return slot_listdef get_text(template): rule = r'::::(.*?)\n' slot_list = re.findall(rule, template) return slot_listdef get_list_or_dict(path): f = open(path, encoding='utf8', errors='ignore') a = f.read() dict_or_list = eval(a) f.close() return dict_or_listlist_id = get_list_or_dict('list_id.txt')print('list_id长度' + str(len(list_id)))f_id_hash_final = open("id_hash_final.txt", 'w', encoding='utf8', errors='ignore')f = open("final_hashs.txt", encoding='utf8', errors='ignore')dict_id_hash = &#123;&#125;while 1: line_txt = f.readline() if not line_txt: break else: dict_id_hash[get_id(line_txt)[0]] = get_text(line_txt)[0]f.close()print('dict_id_hash长度' + str(len(dict_id_hash))) # 121117f_dict_id_hash = open("dict_id_hash.txt", 'w', encoding='utf8', errors='ignore')f_dict_id_hash.write(str(dict_id_hash))f_dict_id_hash.close()for id in list_id: id = id.replace('.mp4', '') f_id_hash_final.write(id + '::::' + dict_id_hash[id] + '\n')f_id_hash_final.close()f_id_hash_final = open("id_hash_final.txt", encoding='utf8', errors='ignore')f_count_hash = open("count_hash.txt", 'w', encoding='utf8', errors='ignore')def subString1(template): # HashTag以‘#’开头，以空格或回车结尾 copy = False finished = False slotList = [] str = "" for s in template: if s == '#': copy = True elif s == ' ': copy = False finished = True elif s == '\n': copy = False finished = True elif copy: str = str + s if finished: if str != "": slotList.append(str) str = "" finished = False return slotListslotList = []i = 1# for i in range(0,3):while 1: line_id_hash = f_id_hash_final.readline() if not line_id_hash: break else: # print() slotList += subString1(line_id_hash) # 将所有HashTag存进list# print(slotList)result = collections.Counter(slotList) # 词频统计# ss = str(result) #词频统计结果转换成字符串并存文件# f_handle_hash.write(ss)# print(result.most_common(100)) 查看统计结果前100名for each in result.most_common(): ss = str(each) # 词频统计结果转换成字符串并存文件 f_count_hash.write(ss + '\n') # print(ss)f_id_hash_final.close()f_count_hash.close() 截取指定长度数据根据词频统计截取hash词频不小于5的数据存入id_hash count_num_hash.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import redef getText(template): rule = r'::::(.*?)\n' slotList = re.findall(rule, template) return slotListdef get_list_or_dict(path): f = open(path, encoding='utf8', errors='ignore') a = f.read() dict_or_list = eval(a) f.close() return dict_or_listf_count_hash = open("count_hash.txt", encoding='utf8', errors='ignore')# f_list_count_hash = open("list_count_hash.txt",'w',encoding='utf8', errors='ignore')list_hash = []flag_hash_line = 0while 1: line_hash = f_count_hash.readline() if not line_hash: break else: rule1 = r"'(.*?)'" rule2 = r'"(.*?)"' rule3 = r' (.*?)\)' try: slotList = re.findall(rule1, line_hash) hash_num = re.findall(rule3, line_hash) if int(hash_num[0]) == 4: break else: list_hash.append(str(slotList[0])) flag_hash_line += 1 except IndexError: slotList = re.findall(rule2, line_hash) list_hash.append(str(slotList[0])) flag_hash_line += 1print(flag_hash_line)list_id = get_list_or_dict('list_id.txt')print(len(list_id))dict_id_hash = get_list_or_dict('dict_id_hash.txt')print(len(dict_id_hash))f_id_hash = open("id_hash", 'w', encoding='utf8', errors='ignore')i = 0for id in list_id: id = str(id).replace('.mp4', '') # print(id) line_list_hash = dict_id_hash[id].split(' ') # print(line_list_hash) line_hash_new = '' for hash in line_list_hash: hash = str(hash).replace('#', '') if hash in list_hash: line_hash_new += '#' + hash + ' ' if line_hash_new != '': # print('空：' + id) # else: f_id_hash.write(id+'::::'+line_hash_new+'\n') i += 1 if (i % 10000 == 0): print(i)f_id_hash.close()print(i) 最终处理获得118684长度的id_hash文件 清理数据根据id_error文件从119120条分割后文件集中删除不合法数据 clean_extract_data.py12345678910111213141516171819202122232425262728293031323334353637383940414243import reimport osimport shutildef get_id(template): rule = r'(.*?)::::' slot_list = re.findall(rule, template) return slot_listdef get_list_or_dict(path): f = open(path, encoding='utf8', errors='ignore') a = f.read() dict_or_list = eval(a) f.close() return dict_or_listdict_id_num = get_list_or_dict('dict_id_num.txt')print(len(dict_id_num))list_id_error = []f_id_error = open('id_error', encoding='utf8', errors='ignore')while 1: line_id_error = f_id_error.readline() if not line_id_error: break else: list_id_error.append(dict_id_num[line_id_error.replace('\n','')])print(len(list_id_error))path = '/home/caoda/Hodge_work_space/dataset/dataset'ii = 0for i in range(1, 16): list_extract_num = os.listdir(path + str(i)) for num in list_extract_num: if(num in list_id_error): try: shutil.rmtree(path+str(i)+'/'+num) # print(path+str(i)+'/'+num) except: print('删除出错') ii += len(os.listdir(path + str(i)))print(ii) 到目前为止数据集已准备并分割完毕。数据集与代码，发布在github.]]></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>字符串处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python函数库]]></title>
    <url>%2F2017%2F12%2F21%2Fcode-base%2F</url>
    <content type="text"><![CDATA[字符串截取字符串截取指定两个符号间的字符串返回所有符合结果的list12345678910111213141516171819202122def get_list_hash(template): # HashTag以‘#’开头，以空格或回车结尾 copy = False finished = False slotList = [] str = "" for s in template: if s == '#': copy = True elif s == ' ': copy = False finished = True elif s == '\n': copy = False finished = True elif copy: str = str + s if finished: if str != "": slotList.append(str) str = "" finished = False return slotList 1234def get_text(template): rule = r':::(.*?)\n' slot_list = re.findall(rule, template) return slot_list 判断是否为英文字母1234567#解决.isalpha()方法对（unicode string，string.isalpha会根据字符串中的字符是否属于Unicode编码的LETTER区域来判断是否都由字母组成。# 所以得出的结果为True，不一定表示只有26个英文字母。）def is_alpha(word): try: return word.encode('ascii').isalpha() except UnicodeEncodeError: return False 文件操作创建文件夹1234567891011121314151617181920212223def mkdir(path): # 去除首位空格 path = path.strip() # 去除尾部 \ 符号 path = path.rstrip("/") # 判断路径是否存在 # 存在 True # 不存在 False isExists = os.path.exists(path) # 判断结果 if not isExists: # 如果不存在则创建目录 # 创建目录操作函数 os.makedirs(path) print(str(path) + ' 创建成功') return True else: # 如果目录存在则不创建，并提示目录已存在 print(str(path) + ' 目录已存在') return False 读取文件读取并按行遍历文件1234567f_video_text = open("video_text.txt",encoding='utf8', errors='ignore')while 1: line_video_text = f_video_text.readline() if not line_video_text: break else: do something 读取文件夹下所有文件名，返回list1234def get_list_file_name(file_dir): # 获取视频名list for root, dirs, files in os.walk(file_dir): continue return files 获取文件行数12345678910def get_lines(file_name): count = 0 thefile = open(file_name, encoding='utf8', errors='ignore') while True: buffer = thefile.read(1024 * 8192) if not buffer: break count += buffer.count('\n') thefile.close() return count 读取文件dict or list123456def get_list_or_dict(path): f = open(path,encoding='utf8', errors='ignore') a = f.read() dict_or_list = eval(a) f.close() return dict_or_list txt文件数据写成dict or list文件1234567891011121314def write_list_or_dict(path_txt,path_dict_or_list_txt): f = open(path_txt,encoding='utf8', errors='ignore') while 1: line_txt = f.readline() if not line_txt: break else: handle list_or_dict[id] = content f.close() f = open(path_dict_or_list_txt, 'w' , encoding='utf8' , errors='ignore') f.write(str(list_or_dict)) f.close print('写入成功') 获取视频或音频时长获取视频或音频时长，需要安装ffmpeg12345678def get_time(filename): command = 'ffprobe -loglevel quiet -print_format json -show_format -show_streams -i ' + filename result = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out = result.stdout.read() temp = str(out.decode('utf-8')) data = json.loads(temp)["format"]['duration'] return data]]></content>
      <categories>
        <category>函数库</category>
      </categories>
      <tags>
        <tag>函数</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络入门]]></title>
    <url>%2F2017%2F12%2F14%2FMLP%2F</url>
    <content type="text"><![CDATA[前言通常构建一个神经网络，有如下步骤1、构建好网络模型2、参数初始化3、前向传播4、计算损失5、反向传播求出梯度6、更新权重以下是快速搭建法（Variable、Sequential、nn）完成的一个自定义两层感知机模型 多层感知机.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import torchfrom torch.autograd import Variableimport torch.nn as nn# 一定要继承 nn.Moduleclass TwoLayerNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(TwoLayerNet, self).__init__() """ 我们在构建模型的时候，能够使用nn.Sequential的地方，尽量使用它，因为这样可以让结构更加清晰 """ self.twolayernet = nn.Sequential( nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, output_size), ) def forward(self, x): """ 在forward函数中，我们会接受一个Variable，然后我们也会返回一个Varible """ y_pred = self.twolayernet(x) return y_pred# M是样本数量，input_size是输入层大小# hidden_size是隐含层大小，output_size是输出层大小M, input_size, hidden_size, output_size = 64, 1000, 100, 10# 生成随机数当作样本，同时用Variable 来包装这些数据，设置 requires_grad=False 表示在方向传播的时候，# 我们不需要求这几个 Variable 的导数x = Variable(torch.randn(M, input_size))y = Variable(torch.randn(M, output_size))model = TwoLayerNet(input_size, hidden_size, output_size)# 定义损失函数loss_fn = nn.MSELoss(size_average=False)## 设置超参数 ##learning_rate = 1e-4EPOCH = 300# 使用optim包来定义优化算法，可以自动的帮我们对模型的参数进行梯度更新。这里我们使用的是随机梯度下降法。# 第一个传入的参数是告诉优化器，我们需要进行梯度更新的Variable 是哪些，# 第二个参数就是学习速率了。optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)## 开始训练 ##for t in range(EPOCH): # 向前传播 y_pred = model(x) # 计算损失 loss = loss_fn(y_pred, y) # 显示损失 if (t + 1) % 50 == 0: print(loss.data[0]) # 在我们进行梯度更新之前，先使用optimier对象提供的清除已经积累的梯度。 optimizer.zero_grad() # 计算梯度 loss.backward() # 更新梯度 optimizer.step()]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>MLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[视频处理]]></title>
    <url>%2F2017%2F12%2F14%2Fhandle-video%2F</url>
    <content type="text"><![CDATA[需求视频下载下来后，需要对每个视频提取12个关键帧，提取每个视频的音频并等分为6段，获取每个视频的text。三者分别保存在以id命名的文件夹下以images/audios/texts命名的文件夹下。 准备工作扫描存放所有视频的文件夹，将所有视频名存储为list_videos_name并写入文件list_videos.txt get_videos_list.py1234567891011121314# 获取要提取关键帧的所有视频的名字存储在list_videos_name并写入文件list_videos_name.txtimport osdef file_name(file_dir): # 获取视频名list for root, dirs, files in os.walk(file_dir): continue return filesvideos_path = 'D:/videos/videos_20171207' # 填写存放视频的文件夹路径list_videos_name = file_name(videos_path)f_list_videos_name = open("list_videos.txt", 'w', encoding='utf8', errors='ignore')f_list_videos_name.write(str(list_videos_name))f_list_videos_name.close() 为了获取视频text，须对之前处理获得的final_texts.txt读取获得dict_id_text并存储为dict_id_text.txt get_dict_id_text.py123456789101112131415161718192021222324252627# 读取final_texts.txt，生成字典dict_id_textimport ref_id_texts = open("final_texts.txt", encoding='utf8', errors='ignore')def get_text(template): rule = r'::::(.*?)\n' slot_list = re.findall(rule, template) return slot_listdef get_id(template): rule = r'(.*?)::::' slot_list = re.findall(rule, template) return slot_listdict_id_text = &#123;&#125;while 1: line_id_text = f_id_texts.readline() if not line_id_text: break else: dict_id_text[str(get_id(line_id_text)[0])] = str(get_text(line_id_text)[0])f_id_texts.close()f_id_texts = open("dict_id_text.txt", 'w', encoding='utf8', errors='ignore')f_id_texts.write(str(dict_id_text)) 细节 因为要处理的数据量较大，要考虑到程序可随时终止再继续运行。故加入四个日志文件 extract_ok.txt：记录所有处理成功的视频 extract_ok_this_time.txt：记录每次任务处理成功的视频 video_error.txt：记录所有处理失败的视频 video_error_this_time.txt：记录每次任务处理失败的视频 在每次任务开始处理视频前将日志文件整合并将处理成功的视频从list_videos中删除即可 视频帧提取用的cv2包，视频的音频提取和音频的分割用的第三方工具ffmpeg 音频分割后发现每段音频间有重叠部分，故应采用精准分割1ffmpeg -y -vn -ss start_time -t 持续时间(00:00:00.900) -i 原文件路径 -acodec copy 生成文件路径 get_final_dataset.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183# 配置video_path、mkpath_images和mkpath_audios内容import cv2import osimport subprocessimport jsondef mkdir(path): # 去除首位空格 path = path.strip() # 去除尾部 \ 符号 path = path.rstrip("/") # 判断路径是否存在 # 存在 True # 不存在 False isExists = os.path.exists(path) # 判断结果 if not isExists: # 如果不存在则创建目录 # 创建目录操作函数 os.makedirs(path) print(str(path) + ' 创建成功') return True else: # 如果目录存在则不创建，并提示目录已存在 print(str(path) + ' 目录已存在') return Falsedef get_lines(file_name): #获取文件行数 count = 0 thefile = open(file_name, encoding='utf8', errors='ignore') while True: buffer = thefile.read(1024 * 8192) if not buffer: break count += buffer.count('\n') thefile.close() return countdef get_time(filename): #获取视频或音频时长 command = ["ffprobe.exe", "-loglevel", "quiet", "-print_format", "json", "-show_format", "-show_streams", "-i", filename] result = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out = result.stdout.read() temp = str(out.decode('utf-8')) data = json.loads(temp)["format"]['duration'] return datadef get_flag_list(flag): #获取flag每一位上数字 list_flag = [] while flag: get_num = flag % 10 list_flag.append(int(get_num)) flag = (flag - get_num) / 10 return list_flagf_list_videos = open("list_videos.txt", encoding='utf8', errors='ignore')a = f_list_videos.read()list_videos = eval(a)f_list_videos.close()print('list_videos.txt读取成功')f_extract_ok = open("extract_ok.txt", 'a', encoding='utf8', errors='ignore')f_extract_ok_this_time = open("extract_ok_this_time.txt", encoding='utf8', errors='ignore')while 1: line_extract_ok_this_time = f_extract_ok_this_time.readline() if not line_extract_ok_this_time: break else: list_videos.remove(str(line_extract_ok_this_time).replace('\n', '') + '.mp4') # 更新未抽取videos f_extract_ok.write(str(line_extract_ok_this_time)) # 更新已抽取videosf_extract_ok_this_time.close()f_extract_ok.close()f_list_videos = open("list_videos.txt", 'w', encoding='utf8', errors='ignore')f_list_videos.write(str(list_videos))f_list_videos.close()print("list_videos清理已抽取视频id成功")print("extract_ok添加已抽取视频id成功")print("共剩余" + str(len(list_videos)) + '条未抽取')print("总共已抽取" + str(get_lines('extract_ok.txt')) + '条视频')f = open("extract_ok_this_time.txt", 'w', encoding='utf8', errors='ignore') # 释放空间f.close()f_video_error = open("video_error.txt", 'a', encoding='utf8', errors='ignore')f_video_error_this_time = open("video_error_this_time.txt", encoding='utf8', errors='ignore')while 1: line_video_error_this_time = f_video_error_this_time.readline() if not line_video_error_this_time: break else: f_video_error.write(str(line_video_error_this_time) + '\n') # 更新失败videosf_video_error_this_time.close()f_video_error.close()print("video_error增加错误videos成功")print("共有" + str(get_lines('video_error.txt')) + '条视频抽取错误')f = open("video_error_this_time.txt", 'w', encoding='utf8', errors='ignore') # 释放空间.f.close()f = open("dict_id_text.txt", encoding='utf8', errors='ignore')a = f.read()dict_id_text = eval(a)f.close()print("dict_id_text读取成功\n")print('---开始抽取---')flag = 1for video_name in list_videos: try: video_path = 'D:/videos/videos_20171207/' + str(video_name) # 配置视频文件夹路径 video_cap = cv2.VideoCapture(video_path) frame_count = 0 all_frames = [] while (True): ret, frame = video_cap.read() if ret is False: break all_frames.append(frame) # 视频所有帧存list frame_count = frame_count + 1 # 帧数 i = 0 flag = int(frame_count / 12) video_name = video_name.replace(".mp4", "") mkpath_images = "I:/images/" + video_name + '/images' # 路径全英文---存12帧图片文件夹路径 mkdir(mkpath_images) mkpath_audios = "I:/images/" + video_name + '/audios' # 路径全英文---存音频文件夹路径 mkdir(mkpath_audios) mkpath_texts = "I:/images/" + video_name + '/texts' # 路径全英文---存文本文件夹路径 mkdir(mkpath_texts) for frame in all_frames: i = i + 1 if (i % flag == 0 and i &lt;= flag * 12): path = mkpath_images + '/' + str(int(i / flag)) + '.jpg' cv2.imwrite(path, frame) # 存储为图像 audio_path = mkpath_audios + '/' + video_name + '.mp3' cmd = 'ffmpeg -i ' + video_path + ' -f mp3 -vn ' + audio_path + ' -loglevel quiet -y' os.system(cmd) # 提取音频 video_time = get_time(video_path) # 音频分段 video_time = int(float(video_time) * 1000) flag = int(video_time / 6) list_flag = get_flag_list(flag) every_time = str(list_flag[2]) + str(list_flag[1]) + str(list_flag[0]) start = 0 end = flag for i in range(1, 7): cmd = 'ffmpeg -y -vn -ss 00:00:0' + str(int(start / 1000)) + '.' + str( int(start % 1000)) + ' -t 00:00:0' + str( int( flag / 1000)) + '.' + every_time + ' -i ' + audio_path + ' -codec copy ' + mkpath_audios + '/' + str( i) + '.mp3' + ' -loglevel quiet' os.system(cmd) start += flag end += flag f_video_text = open(mkpath_texts+'/text.txt', 'w', encoding='utf8', errors='ignore') f_video_text.write(str(dict_id_text[video_name])+'\n') f_video_text.close() print('。。。。。') f_extract_ok_this_time = open("extract_ok_this_time.txt", 'a', encoding='utf8', errors='ignore') f_extract_ok_this_time.write(str(video_name) + '\n') f_extract_ok_this_time.close() flag += 1 except (TimeoutError): print('----video打开失败----') f_video_error_this_time = open("video_error_this_time.txt", 'a', encoding='utf8', errors='ignore') f_video_error_this_time.write(str(video_name) + '\n') f_video_error_this_time.close() print('本次已抽取' + str(get_lines('extract_ok_this_time.txt')) + '条，本次抽取videos错误' + str( get_lines('video_error_this_time.txt')) + '条') # print(dict_video_error_this_time) print('已保存并跳过此条video_id，抽取继续--&gt;') if (flag % 1000 == 0): print('本次已抽取' + flag + '条视频')print("****下载抽取完成****") 小结到目前为止，读取所有的视频并保存其帧、音频、文本数据。数据集与代码，发布在github.]]></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
        <tag>视频处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快捷键]]></title>
    <url>%2F2017%2F12%2F07%2Fshort-key%2F</url>
    <content type="text"><![CDATA[Sublime 快捷键 功能 Ctrl+G 跳转到第几行 Ctrl+W 关闭当前打开文件 Ctrl+Shift+上下键 替换行 Ctrl+Shift+W 关闭所有打开文件 Ctrl+L 选择行，重复可依次增加选择下一行 Ctrl+Shift+L 选择多行 Ctrl+Shift+Enter 在当前行前插入新行 Ctrl+X 删除当前行 Ctrl+M 跳转到对应括号 Ctrl+Shift+M 选中当前括号内容，重复可选着括号本身 Ctrl+N 新建窗口 Ctrl+K+B 开关侧栏 Ctrl+Shift+P 打开命令面板 Ctrl+/ 注释当前行 Ctrl+Shift+/ 当前位置插入注释 Ctrl+Shift+F 查找并替换 F11 全屏 Alt+数字 切换打开第N个文件 Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑 Ctrl+Shift+[ 选中代码，按下快捷键，折叠代码 Ctrl+Shift+] 选中代码，按下快捷键，展开代码 Ctrl+← 向左单位性地移动光标，快速移动光标 Ctrl+Shift+← 向左单位性地选中文本 Ctrl+Shift+D 复制光标所在整行，插入到下一行 Eclipse(自定义) 快捷键 功能 Alt+/ 代码辅助 Ctrl+1 快速修复 Ctrl+l 跳转到第几行 Ctrl+W 关闭当前打开文件 Alt+上下键 替换行 Ctrl+Shift+W 关闭所有打开文件 Ctrl+D 删除当前行 Ctrl+/ 注释当前行 Ctrl+← 向左单位性地移动光标，快速移动光标 Ctrl+Shift+← 向左单位性地选中文本 Ctrl+Alt+D 复制光标所在整行，插入到下一行 Windows 快捷键 功能 win+A 打开活动中心 win+I 打开电脑设置 win+K 打开连接面板，连接到无线显示和音频设备 win+加号键 放大 win+SHIFT+↑ 垂直最大化活动窗口，保持当前宽度 win+CTRL+D 创建一个新的虚拟桌面 win+CTRL+F4 关闭当前的虚拟桌面 Chrome 快捷键 功能 ctrl+T 打开新标签页 ctrl+Tab 下一个标签页 alt+D 定焦到地址栏]]></content>
      <categories>
        <category>参考手册</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vine数据集处理3]]></title>
    <url>%2F2017%2F12%2F07%2Fvinedataset3%2F</url>
    <content type="text"><![CDATA[需求现在拥有了一个121117条 id::::url 的final_hashs.txt文件需要读取里面的url并下载对应的视频，有如下要求： 随时可手动停止下载，再次开始下载可继续上次任务 保存下载成功的视频id，保存下载失败(链接失效或下载超时)的视频id::::url 自定义下载路径download_2.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#可间断性地下载final_urls.txt的链接获得视频集import urllib.requestdef getLines(file_name): #获取文件行数 count = 0 thefile = open(file_name, encoding='utf8', errors='ignore') while True: buffer = thefile.read(1024 * 8192) if not buffer: break count += buffer.count('\n') thefile.close() return countdict_id_url = &#123;&#125;f = open("dict_id_url.txt",encoding='utf8', errors='ignore')a = f.read()dict_id_url = eval(a)f.close()print("dict_id_url读取成功") #读取剩余urlsf_down_ok = open("down_ok.txt",'a',encoding='utf8', errors='ignore')f_down_ok_this_time = open("down_ok_this_time.txt",encoding='utf8', errors='ignore')while 1: line_down_ok_this_time = f_down_ok_this_time.readline() if not line_down_ok_this_time: break else: line_down_ok_this_time = line_down_ok_this_time.replace('\n','') del dict_id_url[str(line_down_ok_this_time)] #更新未下载urls f_down_ok.write(str(line_down_ok_this_time)+'\n') #更新已下载urlsf_down_ok_this_time.close()f_down_ok.close()f_dict_id_url = open("dict_id_url.txt",'w',encoding='utf8', errors='ignore') #保存未下载的urlsf_dict_id_url.write(str(dict_id_url))f_dict_id_url.close()print("dict_id_url清理已下载urls成功")print("down_ok添加已下载urls成功")print("共剩余"+str(len(dict_id_url))+'条未下载')print("总共已下载"+str(getLines('down_ok.txt'))+'条视频')f = open("down_ok_this_time.txt",'w',encoding='utf8', errors='ignore') #清空文本f.close()f_url_error = open("url_error.txt",'a',encoding='utf8', errors='ignore')f_url_error_this_time = open("url_error_this_time.txt",encoding='utf8', errors='ignore')while 1: line_url_error_this_time = f_url_error_this_time.readline() if not line_url_error_this_time: break else: f_url_error.write(str(line_url_error_this_time)) #更新失效urlsf_url_error_this_time.close()f_url_error.close()print("url_error增加错误urls成功")print("共有"+str(getLines('url_error.txt'))+'条视频链接错误')f = open("url_error_this_time.txt",'w',encoding='utf8', errors='ignore') #清空文本f.close()print('---开始下载---')flag = 1for key in dict_id_url: url = str(dict_id_url[key]) try: if '.mp4?' in url: video_name = str(key)+'.mp4' #视频保存路径 urllib.request.urlretrieve(url, video_name) #下载 f_down_ok_this_time = open("down_ok_this_time.txt", 'a', encoding='utf8', errors='ignore') f_down_ok_this_time.write(str(key)+'\n') f_down_ok_this_time.close() print('。。。。。') #下载成功一条打一个标记 flag += 1 else: f_url_error_this_time = open("url_error_this_time.txt", 'a', encoding='utf8', errors='ignore') f_url_error_this_time.write(str(key)+'::::'+dict_id_url[key]+'\n') f_url_error_this_time.close() except (TimeoutError,urllib.error.URLError): print('----url失效，下载超时----') f_url_error_this_time = open("url_error_this_time.txt", 'a', encoding='utf8', errors='ignore') f_url_error_this_time.write(str(key) + '::::' + dict_id_url[key] + '\n') f_url_error_this_time.close() print('本次已下载'+str(getLines('down_ok_this_time.txt'))+'条，本次下载url错误'+str(getLines('url_error_this_time.txt'))+'条') print('已保存并跳过此条url，下载继续--&gt;') if(flag%1000==0): print('本次已下载'+flag+'条视频')print("****下载全部完成****") 小结数据集与代码，发布在github.]]></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>文件读取</tag>
        <tag>下载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vine数据集处理2]]></title>
    <url>%2F2017%2F12%2F04%2Fvine-data-2%2F</url>
    <content type="text"><![CDATA[需求现在拥有了一一对应的id::::text（video_text_hash.txt）和id::::url（video_url_hash.txt）。需求是永远递增的嘛~~~将video_text_hash.txt中text内容的#hash拿出来，单独存成id::::text（id_text.txt）和id::::hash（id_hash.txt）PS:拿完#hash后剩下的text若为空则删除此条目 handle_text.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import re#根据video_text_hash.txt将hash提取出来获得id_hash.txt和剩下的id_text.txt其中排除剩余为空的id_text_void.txtf_video_texts = open("video_text_hash.txt",encoding='utf8', errors='ignore')f_text = open("id_text.txt",'w',encoding='utf8', errors='ignore')f_text_void = open("id_text_void.txt",'w',encoding='utf8', errors='ignore')f_hash = open("id_hash.txt",'w',encoding='utf8', errors='ignore')def get_hashs(template): # HashTag以‘#’开头，以空格或回车结尾 copy = False finished = False slotList = [] str = "" for s in template: if s == '#': copy = True elif s == ' ': copy = False finished = True elif s == '\n': copy = False finished = True elif copy: str = str + s if finished: if str != "": slotList.append(str) str = "" finished = False return slotListdef getText(template): rule = r'::::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?)::::' slotList = re.findall(rule, template) return slotList# for i in range(1,20):ii=0while 1: line_video_text = f_video_texts.readline() list_hash = [] if not line_video_text: break else: try: list_hash = get_hashs(line_video_text) #hash组成一个list list_text = getText(line_video_text) #text组成list str_text = str(list_text[0])+'\n' str_hashs = '' for hash in list_hash: #遍历每句的每个hash str_hash1 = '#'+str(hash)+' ' str_hashs += str(str_hash1) str_hash2 = '#' + str(hash) + '\n' #还原hash str_text = str_text.replace(str_hash1,"") #删除hash str_text = str_text.replace(str_hash2, "\n") except IndexError: print("error") if(str_text != '\n'): f_text.write(str(getId(line_video_text)[0])+'::::'+str_text) f_hash.write(str(getId(line_video_text)[0])+'::::'+str_hashs+'\n') else: f_text_void.write(str(getId(line_video_text)[0])+'\n') ii+=1 if(ii%10000==0): print(ii)f_video_texts.close()f_text.close()f_text_void.close()f_hash.close() text处理对于id_text.txt里的text，还需要做以下操作，1、去除非英文字符。2、去除停顿词。3、小写化text。得到id_text_final.txt和被剔除的剩余为空的id存到id_text_final_void.txt。同时顺便得到清理后的id::::hashs文件id_hash_final.txt（顺便小写化） handle_text_final.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import re#根据id_text.txt做文本处理获得id_text_final.txt和排除剩余为空的id_text_final_void.txt#顺便得到清理后的id::::hashs文件id_hash_final.txtf_video_texts = open("id_text.txt",encoding='utf8', errors='ignore')f_stop_word = open("stop_word.txt",encoding='utf8', errors='ignore')f_id_hash = open("id_hash.txt",encoding='utf8', errors='ignore')f_id_text_final = open("id_text_final.txt",'w',encoding='utf8', errors='ignore')f_id_text_final_void = open("id_text_final_void.txt",'w',encoding='utf8', errors='ignore')f_id_hash_final = open("id_hash_final.txt",'w',encoding='utf8', errors='ignore')def getText(template): rule = r'::::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?)::::' slotList = re.findall(rule, template) return slotList#解决.isalpha()方法对（unicode string，string.isalpha会根据字符串中的字符是否属于Unicode编码的LETTER区域来判断是否都由字母组成。# 所以得出的结果为True，不一定表示只有26个英文字母。）def isAlpha(word): try: return word.encode('ascii').isalpha() except UnicodeEncodeError: return Falselist_stop_word = []while 1: line_stop_word = f_stop_word.readline() if not line_stop_word: break else: list_stop_word.append(str(line_stop_word).replace("\n",'').lower())print("停用词列表生成成功")dict_hash = &#123;&#125;while 1: line_hash = f_id_hash.readline() if not line_hash: break else: dict_hash[getId(line_hash)[0]] = getText(line_hash)[0]print("hash字典生成成功")ii=0# for i in range(1,3):while 1: line_video_text = f_video_texts.readline() list_word = [] if not line_video_text: break else: try: list_text = str(getText(line_video_text)[0]) text_final = '' for s in list_text: if isAlpha(s): text_final += s elif s==' ': text_final += s #text字符串，包含多余空格 # print(text_final) list_word = text_final.split(' ') #分词，包含空字符 text_final = '' for word in list_word: if word == '': continue elif word not in list_stop_word: text_final += word+' ' if text_final != '': str_hash = dict_hash[getId(line_video_text)[0]] #清除非全英文字母的hash rule = r'#(.*?) ' list_hashs = re.findall(rule, str_hash) str_hash = '' for hash in list_hashs: flag = 1 for s in hash: if isAlpha(s): continue else: flag = 0 break if flag: str_hash += '#'+hash+' ' if str_hash != '': f_id_text_final.write(getId(line_video_text)[0]+'::::'+text_final+'\n') #将去除空结果的id::::text写入id_text_final.txt f_id_hash_final.write(getId(line_video_text)[0]+'::::'+str_hash+'\n') else: f_id_text_final_void.write(getId(line_video_text)[0]+'\n') #将空白结果id写入text_final_void.txt except IndexError: print("error") ii+=1 if(ii%10000==0): print(ii)# print(len(list_1)) #data1的id listf_video_texts.close()f_stop_word.close()f_id_hash.close()f_id_text_final.close()f_id_text_final_void.close()f_id_hash_final.close() 统计hash词频根据id_hash_final.txt做hash的词频统计得到count_hash.txt count.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 统计hash总数与词频import collectionsimport ref_id_hash_final = open("id_hash_final.txt", encoding='utf8', errors='ignore')f_count_hash = open("count_hash.txt", 'w', encoding='utf8', errors='ignore')def subString1(template): # HashTag以‘#’开头，以空格或回车结尾 copy = False finished = False slotList = [] str = "" for s in template: if s == '#': copy = True elif s == ' ': copy = False finished = True elif copy: str = str + s if finished: if str != "": slotList.append(str) str = "" finished = False return slotListslotList = []i=1# for i in range(0,5):while 1: line_id_hash = f_id_hash_final.readline() if not line_id_hash: break else: slotList += subString1(line_id_hash) #将所有HashTag存进list# print(slotList)result = collections.Counter(slotList) #词频统计#ss = str(result) #词频统计结果转换成字符串并存文件# f_handle_hash.write(ss)#print(result.most_common(100)) 查看统计结果前100名for each in result.most_common(): ss = str(each) #词频统计结果转换成字符串并存文件 f_count_hash.write(ss+'\n') #print(ss)f_id_hash_final.close()f_count_hash.close() 截取指定长度数据集遍历id_hash_final.txt里的hash，走到最后一个频率为5的hash（premiosjuventud）停止首先生成对应的list与dict以供快速读取 get_list_or_dict.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import re#id_text_final--&gt;dict_id_text_final#id_hash_final--&gt;dict_id_hash_final#count_hash--&gt;list_count_hash#video_url_hash--&gt;dict_id_urlf_id_text_final = open("id_text_final.txt",encoding='utf8', errors='ignore')f_dict_id_text_final = open("dict_id_text_final.txt",'w',encoding='utf8', errors='ignore')f_id_hash_final = open("id_hash_final.txt",encoding='utf8', errors='ignore')f_dict_id_hash_final = open("dict_id_hash_final.txt",'w',encoding='utf8', errors='ignore')f_count_hash = open("count_hash.txt",encoding='utf8', errors='ignore')f_list_count_hash = open("list_count_hash.txt",'w',encoding='utf8', errors='ignore')f_video_url_hash = open("video_url_hash.txt",encoding='utf8', errors='ignore')f_dict_id_url = open("dict_id_url.txt",'w',encoding='utf8', errors='ignore')def getText(template): rule = r'::::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?)::::' slotList = re.findall(rule, template) return slotListlist_hash = []while 1: line_hash = f_count_hash.readline() if not line_hash: break else: rule1 = r"'(.*?)'" rule2 = r'"(.*?)"' try: slotList = re.findall(rule1, line_hash) list_hash.append(str(slotList[0])) except IndexError: slotList = re.findall(rule2, line_hash) list_hash.append(str(slotList[0]))f_list_count_hash.write(str(list_hash))# print(len(list_1)) #data1的id listdict_id_text_final = &#123;&#125;dict_id_hash_final = &#123;&#125;ii=1while 1: line_id_text = f_id_text_final.readline() line_id_hash = f_id_hash_final.readline() if not line_id_text: break else: each_id = getId(line_id_text) each_text = getText(line_id_text) dict_id_text_final[each_id[0]] = each_text[0] each_id = getId(line_id_hash) each_text = getText(line_id_hash) dict_id_hash_final[each_id[0]] = each_text[0] ii += 1 if (ii % 50000 == 0): print(ii)f_dict_id_text_final.write(str(dict_id_text_final))f_dict_id_hash_final.write(str(dict_id_hash_final))# print(dict_id_text_final['980744748887429120']) #data2的dict &#123;id:text&#125;# print(dict_id_hash_final['980744748887429120'])dict_id_url = &#123;&#125;ii=1while 1: line_id_url = f_video_url_hash.readline() if not line_id_url: break else: each_id = getId(line_id_url) each_text = getText(line_id_url) dict_id_url[each_id[0]] = each_text[0] ii += 1 if (ii % 50000 == 0): print(ii)f_dict_id_url.write(str(dict_id_url))# print(dict_id_url['980744748887429120']) #data2的dict &#123;id:text&#125;f_id_text_final.close()f_dict_id_text_final.close()f_id_hash_final.close()f_dict_id_hash_final.close()f_count_hash.close()f_list_count_hash.close()f_video_url_hash.close()f_dict_id_url.close() 最后得到一定数量的数据集final_id_texts/hashs/urls.txt get_final_urls.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import re#根据count_hash.txt和id_hash_final.txt获得最终需要的一定数量的数据集final_id_texts/hashs/urls.txtf_dict_id_text_final = open("dict_id_text_final.txt",encoding='utf8', errors='ignore')f_dict_id_hash_final = open("dict_id_hash_final.txt",encoding='utf8', errors='ignore')f_list_count_hash = open("list_count_hash.txt",encoding='utf8', errors='ignore')f_dict_id_url = open("dict_id_url.txt",encoding='utf8', errors='ignore')f_final_texts = open("final_texts.txt",'w',encoding='utf8', errors='ignore')f_final_hashs = open("final_hashs.txt",'w',encoding='utf8', errors='ignore')f_final_urls = open("final_urls.txt",'w',encoding='utf8', errors='ignore')def getText(template): rule = r'::::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?)::::' slotList = re.findall(rule, template) return slotListlist_hash = []f = open("list_count_hash.txt",encoding='utf8', errors='ignore')a = f.read()list_hash = eval(a)f.close()print("list_count_hash读取成功\n")print(len(list_hash))dict_id_text_final = &#123;&#125;f = open("dict_id_text_final.txt",encoding='utf8', errors='ignore')a = f.read()dict_id_text_final = eval(a)f.close()print("dict_id_text_final读取成功\n")dict_id_hash_final = &#123;&#125;f = open("dict_id_hash_final.txt",encoding='utf8', errors='ignore')a = f.read()dict_id_hash_final = eval(a)f.close()print("dict_id_hash_final读取成功\n")dict_id_url = &#123;&#125;f = open("dict_id_url.txt",encoding='utf8', errors='ignore')a = f.read()dict_id_url = eval(a)f.close()print("dict_id_url读取成功\n")print("开始匹配")list_del = []iii=0for hash in list_hash: ii = 0 for key in dict_id_hash_final: if (str("#"+hash+" ") in str(dict_id_hash_final[key])): f_final_texts.write(key + '::::' + dict_id_text_final[key] + '\n') f_final_hashs.write(key + '::::' + dict_id_hash_final[key] + '\n') f_final_urls.write(key + '::::' + dict_id_url[key] + '\n') ii+=1 list_del.append(key) for i_del in list_del: del dict_id_hash_final[i_del] list_del = [] iii+=ii print(str(hash)+"--匹配结束，其写入--"+str(ii)+'--条--共计写入'+str(iii)+'条') if (str(hash) == 'premiosjuventud'): breakf_dict_id_text_final.close()f_dict_id_hash_final.close()f_list_count_hash.close()f_dict_id_url.close()f_final_texts.close()f_final_hashs.close()f_final_urls.close() 再处理hash将final_hashs.txt复制到final_hashs1.txt将每条hashs中出现小于5次的hash删除得到最终final_hashs.txt clean_final_hashs.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#对于final_hashs1.txt每条item清除频率少于5次的hashimport ref_final_hashs1 = open("final_hashs1.txt", encoding='utf8', errors='ignore')f_list_hash = open("count_hash.txt", encoding='utf8', errors='ignore')f_final_hashs = open("final_hashs.txt", 'w', encoding='utf8', errors='ignore')def getText(template): rule = r'::::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?)::::' slotList = re.findall(rule, template) return slotListlist_hash = []# for i in range(0,5):while 1: line_id_hash = f_list_hash.readline() if not line_id_hash: break else: rule = r"'(.*?)'" list_hash += re.findall(rule, line_id_hash)print('list_hash生成成功')# for i in range(0,100):i=0while 1: line_hashs = f_final_hashs1.readline() list_hashs = [] str_hashs = '' if not line_hashs: break else: try: # str_hashs = str(getId(line_hashs)[0]) + '::::' rule = r"#(.*?) " list_hashs += re.findall(rule, line_hashs) for hash in list_hashs: if str(hash) in list_hash: str_hashs += '#' + str(hash) + ' ' if(str_hashs != ''): f_final_hashs.write(str(getId(line_hashs)[0]) + '::::'+str_hashs + '\n') else: print('------') i+=1 except IndexError: print("erro") if(i%10000==0): print(i)f_final_hashs1.close()f_list_hash.close()f_final_hashs.close() 小结到目前为止，获得了121117条一一对应的final_hashs/urls/texts.txt文件数据集与代码，发布在github.]]></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>字符串处理</tag>
        <tag>文件读取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2017%2F12%2F01%2Fgit%2F</url>
    <content type="text"><![CDATA[准备工作下载并安装git软件然后直接可以上手了！！！打开Git Bash，cd到你要创建成版本库的目录（最好是空目录，重要文件目录记得备份）12$ git initInitialized empty Git repository in I:/git/vine_dataset/.git/ 接下来可以在这个文件夹里为所欲为（不要碰默认隐藏的功能文件夹.git） 把一个文件放到Git仓库只需要两步第一步，用命令git add告诉Git，把文件添加到仓库：1git add 修改的文件名 PS:git add xx命令可以将xx文件添加到暂存区，如果有很多改动可以通过git add -A .来一次添加所有改变的文件。注意 -A 选项后面还有一个句点 . git add -A表示添加所有内容， git add . 表示添加新文件和编辑过的文件不包括删除的文件; git add -u 表示添加编辑或者删除的文件，不包括新添加的文件。 第二步，用命令git commit告诉Git，把文件提交到仓库：1$ git commit -m 提交描述 其他命令git status 让我们时刻掌握仓库当前的状态，反馈类似nothing to commit, working tree cleangit diff 文件名 查看具体修改 git log 查看提交历史,简易信息git log --pretty=oneline git reset --hard HEAD^ 回退到某一版本，在Git中，用HEAD表示当前版本，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 git reflog 记录你的每一次命令 git checkout -- file 丢弃工作区的修改 git reset HEAD file 可以把暂存区的修改撤销掉，重新放回工作区 git rm test.txt 删除文件，可用git checkout -- test.txt还原 远程仓库在继续阅读后续内容前，请自行注册GitHub账号。由于你的本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以，需要一点设置第1步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Keyssh-keygen -t rsa -C &quot;youremail@example.com&quot; 你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可，由于这个Key也不是用于军事目的，所以也无需设置密码。如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 第2步：登陆GitHub，打开“settings”，“SSH and GPG Keys”页面添加你的id_rsa.pub内容在GitHub创建一个同名Git仓库，在本地的对应的Git仓库下运行命令12git remote add origin git@github.com:Hodgeli/vine_dataset.gitgit push -u origin master 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令git push origin master 从远程库克隆在github创建新仓库（repository）: cloneTest用命令git clone克隆一个本地库：1$ git clone git@github.com:Hodgeli/cloneTest.git]]></content>
      <categories>
        <category>参考手册</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vine数据集处理]]></title>
    <url>%2F2017%2F11%2F30%2Fhandle-tag%2F</url>
    <content type="text"><![CDATA[问题描述有一个从vine.co爬取下来的数据集，需要对视频描述中的HashTag进行提取与计数 筛选首先筛选视频描述中拥有HashTag（以#开头的单词或短语）的条目 handle_data.py12345678910111213141516171819202122#得到带hash的视频描述与视频链接#解决编码问题，忽略警告f_video_text = open("video_text.txt",encoding='utf8', errors='ignore')f_video_url = open("video_download_link.txt",encoding='utf8', errors='ignore')#要写入的文件f_video_text_hash = open("video_text_hash.txt",'w',encoding='utf8', errors='ignore')f_video_url_hash = open("video_url_hash.txt",'w',encoding='utf8', errors='ignore')e = '#' #以‘#’号标识while 1:#分别读取下一行 line_video_text = f_video_text.readline() line_video_url = f_video_url.readline() if not line_video_text: break#如果包含HashTag if (e in line_video_text): f_video_text_hash.write(line_video_text) f_video_url_hash.write(line_video_url)f_video_url.close()f_video_text.close() 统计遍历得到的文件，提取所有的HashTag并做词频统计 handle_hash.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 统计hash总数与词频import collectionsf_video_text_hash = open("video_text_hash.txt", encoding='utf8', errors='ignore')f_handle_hash = open("handle_hash.txt", 'w', encoding='utf8', errors='ignore')def subString1(template): # HashTag以‘#’开头，以空格或回车结尾 copy = False finished = False slotList = [] str = "" for s in template: if s == '#': copy = True elif s == ' ': copy = False finished = True elif s == '\n': copy = False finished = True elif copy: str = str + s if finished: if str != "": slotList.append(str) str = "" finished = False return slotListslotList = []while 1: line_video_text = f_video_text_hash.readline() if not line_video_text: break else: slotList += subString1(line_video_text) #将所有HashTag存进listresult = collections.Counter(slotList) #词频统计#ss = str(result) #词频统计结果转换成字符串并存文件# f_handle_hash.write(ss)#print(result.most_common(100)) 查看统计结果前100名for each in result.most_common(): ss = str(each) #词频统计结果转换成字符串并存文件 f_handle_hash.write(ss+'\n') #print(ss)f_video_text_hash.close()f_handle_hash.close() 数据集2以上是处理数据集1：n条video:::text对应n条video:::url，顺序数量一一对应。后来又有了数据集2：n条video:::text对应m条video:::url,而且顺序不对应，两者间也不具备完全的包含关系。 筛选首先是获得含有hashtag的数据集合video_text_hash.txt get_video_text_hash.py12345678910111213#得到带hash的视频描述f_video_text = open("location_videos_description",encoding='utf8', errors='ignore')f_video_text_hash = open("video_text_hash.txt",'w',encoding='utf8', errors='ignore')e = '#'while 1: line_video_text = f_video_text.readline() if not line_video_text: break if (e in line_video_text): f_video_text_hash.write(line_video_text)f_video_text.close() 去重然后将这个数据集2的video_text_hash.txt（复制成video_text_hash_2.txt）和数据集1的video_text_hash.txt（复制成video_text_hash_1.txt）比较，去除重复条目得到video_text_hash_3.txt，重复的条目存到video_cover.txt get_video_text_hash_off_cover.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import re#data1与data2的video_text_hash去重f1_video_text = open("video_text_hash_1.txt",encoding='utf8', errors='ignore')f2_video_text = open("video_text_hash_2.txt",encoding='utf8', errors='ignore')f_video_text_hash = open("video_text_hash_3.txt",'w',encoding='utf8', errors='ignore')f_video_text_cover = open("video_cover.txt",'w',encoding='utf8', errors='ignore')def getText(template): rule = r':::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?):::' slotList = re.findall(rule, template) return slotListlist_1 = [] #用list存video_text_hash_1.txt的video的idwhile 1: line_video_text = f1_video_text.readline() if not line_video_text: break else: each_id = getId(line_video_text) list_1.append(each_id[0])#print(list_1) #data1的id listdict_2 = &#123;&#125; #用dict存video_text_hash_2.txt的video的id：texti=1while 1: line_video_text = f2_video_text.readline() if not line_video_text: break else: each_id = getId(line_video_text) each_text = getText(line_video_text) dict_2[each_id[0]] = each_text[0]#print(dict_2) #data2的dict &#123;id:text&#125;for key in dict_2: if(key not in list_1): f_video_text_hash.write(key + '::' + dict_2[key] + '\n') else: f_video_text_cover.write(key + '::' + dict_2[key] + '\n') i+=1 if(i%5000==0): print(i)f1_video_text.close()f2_video_text.close()f_video_text_hash.close() 获得url然后将这个数据集2的video_text_hash_3.txt（复制成video_text_hash_off_cover.txt）和数据集2的video_url.txt比较，找到缺失的url_id条目得到video_url_off.txt，清理的id_url存到video_url_hash.txt get_video_url_hash.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import re#根据video_text_hash_off_cover.txt清洗video_url.txtf1_video_text = open("video_text_hash_off_cover.txt",encoding='utf8', errors='ignore')f2_video_text = open("video_url.txt",encoding='utf8', errors='ignore')f_video_url_hash = open("video_url_hash.txt",'w',encoding='utf8', errors='ignore')f_video_url_off = open("video_url_off.txt",'w',encoding='utf8', errors='ignore')def getText(template): rule = r':::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?):::' slotList = re.findall(rule, template) return slotListlist_1 = []while 1: line_video_text = f1_video_text.readline() if not line_video_text: break else: each_id = getId(line_video_text) list_1.append(each_id[0])#print(list_1) #data1的id listii=1dict_2 = &#123;&#125;while 1: line_video_text = f2_video_text.readline() if not line_video_text: break else: each_id = getId(line_video_text) each_text = getText(line_video_text) dict_2[each_id[0]] = each_text[0] ii += 1 if (ii % 5000 == 0): print(ii)#print(dict_2) #data2的dict &#123;id:text&#125;print("开始清洗")ii=1for i in list_1: if(i in dict_2): try: f_video_url_hash.write(i + '::::' + dict_2[i] + '\n') except IndexError: print("c1") else: try: f_video_url_off.write(i + '\n') except IndexError: print("c2") ii+=1 if(ii%5000==0): print(ii)f1_video_text.close()f2_video_text.close()f_video_url_hash.close() 获得text将video_text_hash_off_cover.txt依据video_url_off.txt清除缺失url的条目得到最终video_text_hash_4.txt get_video_text_hash_clean_again.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import re#将video_text_hash_off_cover.txt依据video_url_off.txt清除缺失url项f1_video_text = open("video_url_off.txt",encoding='utf8', errors='ignore')f2_video_text = open("video_text_hash_off_cover.txt",encoding='utf8', errors='ignore')f_video_text_hash = open("video_text_hash_4.txt",'w',encoding='utf8', errors='ignore')def getText(template): rule = r':::(.*?)\n' slotList = re.findall(rule, template) return slotListdef getId(template): rule = r'(.*?):::' slotList = re.findall(rule, template) return slotListlist_1 = []while 1: line_video_text = f1_video_text.readline() if not line_video_text: break else: #each_id = getId(line_video_text) rule = r'(.*?)\n' slotList = re.findall(rule, line_video_text) list_1.append(slotList[0])#print(list_1) #data1的id listdict_2 = &#123;&#125;list_2 = []ii=1while 1: line_video_text = f2_video_text.readline() if not line_video_text: break else: each_id = getId(line_video_text) each_text = getText(line_video_text) dict_2[each_id[0]] = each_text[0] list_2.append(each_id[0]) ii += 1 if (ii % 10000 == 0): print(ii)#print(list_2[0]) #data2的dict &#123;id:text&#125;ii=0for i in list_2: if(i in list_1): pass else: f_video_text_hash.write(i + ':::' + dict_2[i] + '\n') ii += 1 if (ii % 10000 == 0): print(ii)f1_video_text.close()f2_video_text.close()f_video_text_hash.close() 整合手动将数据集1得到的video:::text和video:::url文件（video_text_hash.txt,video_url_hash.txt），和数据集2得到的video:::text和video:::url文件（video_text_hash_4.txt,video_url_hash.txt）合并得到最终video_text_hash.txt和video_url_hash.txt。 小结目前只处理了这两类数据，数据集中还包含了用户信息之类的其他信息，发布在github.]]></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>字符串处理</tag>
        <tag>文件读取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[h5py]]></title>
    <url>%2F2017%2F11%2F28%2Fh5py%2F</url>
    <content type="text"><![CDATA[h5py文件是存放两类对象的容器，数据集(dataset)和组(group)，dataset类似数组类的数据集合，和numpy的数组差不多。group是像文件夹一样的容器，它好比python中的字典，有键(key)和值(value)。group中可以存放dataset或者其他的group。”键”就是组成员的名称，”值”就是组成员对象本身(组或者数据集)，下面来看下如何创建组和数据集。 基本操作12345678910111213141516import h5pyimport numpy as np#生成文件，要是读取文件的话，就把w换成rf=h5py.File("mydataset.h5","w")#创建数据集d1=f.create_dataset("dset1",(3,4),'i')#赋值d1[...]=np.arange(12).reshape((3,4))#同时创建并赋值f["dset2"]=np.arange(15)#已有numpy数组，创建数据集并赋值a=np.arange(20)d1=f.create_dataset("dset3",data=a)for key in f.keys(): print(f[key].name) print(f[key].value) group1234567891011121314import h5pyimport numpy as npf=h5py.File("myh5py.hdf5","w")#创建一个名字为bar的组g1=f.create_group("bar")#在bar这个组里面分别创建name为dset1,dset2的数据集并赋值。g1["dset1"]=np.arange(10)g1["dset2"]=np.arange(12).reshape((3,4))for key in g1.keys(): print(g1[key].name) print(g1[key].value) 文件读写123456789101112131415161718192021222324import h5pyimport numpy as npfile=h5py.File("TrainSet_rotate.h5","w")data = np.array( [222,333,444] )label = np.array( [0,1,0] )img_num = np.array( [0,1,2] )# 写入file.create_dataset('train_set_x', data = data)file.create_dataset('train_set_y', data = label)file.create_dataset('train_set_num',data = img_num)# 。。。。。。。。。file.close()# 读方式打开文件file=h5py.File('TrainSet_rotate.h5','r')# 尽管后面有 '[:]', 但是矩阵怎么进去的就是怎么出来的，不会被拉长（matlab后遗症）train_set_data = file['train_set_x'][:]train_set_y = file['train_set_y'][:]train_set_img_num = file['train_set_num'][:]# .........file.close()]]></content>
      <categories>
        <category>Python踩坑指南</category>
      </categories>
      <tags>
        <tag>包使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[twitter爬虫]]></title>
    <url>%2F2017%2F11%2F22%2Ftwitter%2F</url>
    <content type="text"><![CDATA[帐号准备 首先需要注册twitter账号，注册页使用google邮箱，下一页验证手机号。 新建一个twitter application 链接：新建项目按要求填写内容后在keys and access token页面获取对应keys。 环境配置安装tweepy： 1pip install tweepy 简单示例1234567891011121314151617# -*- coding: utf-8 -*-import tweepyfrom tweepy import OAuthHandlerconsumer_key = 'replace your own account consumer_key'consumer_secret = 'replace your own account consumer_secret'access_token = 'replace your own account access_token'access_secret = 'replace your own account access_secret'auth = OAuthHandler(consumer_key,consumer_secret)auth.set_access_token(access_token,access_secret)#api = tweepy.API(auth)api = tweepy.API(auth,proxy="127.0.0.1:1080")#代理for status in tweepy.Cursor(api.home_timeline).items(2):#抓取条数 print (status.text) 运行代码获得对应twitter账号的首页推文]]></content>
      <categories>
        <category>工具积累</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2017%2F11%2F18%2FFirst-Blog%2F</url>
    <content type="text"><![CDATA[标题在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶。blog将最高级的标题自动整理生成为目录 水平线连续三个_或-或* 文本样式加粗：用两个*或_包裹文字斜体：用一个*或_包裹文字删除线：用两个~包裹文字转义：在需转义字符前加\ 列表无序列表： Create a list by starting a line with +, -, or * 有序列表： You can use sequential numbers… …or keep all the numbers as 1. 链接行内形式：1This is an [example link](http://example.com/ "With a Title"). This is an example link.参考形式：123456I get 10 times more traffic from [Google][1] than from[Yahoo][2] or [MSN][3].[1]: http://google.com/ "Google"[2]: http://search.yahoo.com/ "Yahoo Search"[3]: http://search.msn.com/ "MSN Search" I get 10 times more traffic from Google than from Yahoo or MSN.]]></content>
      <categories>
        <category>参考手册</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>工具</tag>
      </tags>
  </entry>
</search>
